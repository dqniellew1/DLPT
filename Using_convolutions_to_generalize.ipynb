{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Using convolutions to generalize.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1CkVi73x_T9YI_RHVdh5aZJy-dYEn4C5c",
      "authorship_tag": "ABX9TyOEKo3w9cJzDDuEKnOXC/To",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d9a18bc06db84b15ace14005bee40684": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_6c28d961c946488db408f487ba4583e9",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_e96a24a9b21d40138e9429b51ff90913",
              "IPY_MODEL_db6c57a072324042956dd2598b2b8180"
            ]
          }
        },
        "6c28d961c946488db408f487ba4583e9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e96a24a9b21d40138e9429b51ff90913": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_2019e4fc31ff4843ab4a8062a1da2aa6",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6d77d8c31ea8407a9d47dabe85852ecf"
          }
        },
        "db6c57a072324042956dd2598b2b8180": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_cf65382f39824b9ba4a9ef190eaba3b4",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 170500096/? [01:00&lt;00:00, 2773253.84it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ef3fe1cf1fc546ba826a9c45a0443e10"
          }
        },
        "2019e4fc31ff4843ab4a8062a1da2aa6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6d77d8c31ea8407a9d47dabe85852ecf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "cf65382f39824b9ba4a9ef190eaba3b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ef3fe1cf1fc546ba826a9c45a0443e10": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dqniellew1/DLPT/blob/master/Using_convolutions_to_generalize.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9T3pQNnL7Ud",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "drive_dir = 'drive/My Drive/dlwpt-code/data/'"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ph0WLgkvL-KU",
        "colab_type": "text"
      },
      "source": [
        "Taking a 1D view of our input image and multiplying it with a `n_output_features x n_input_features` weight matrix, as done in `nn.Linear`, means taking all pixels in the image, and for each channel computing a weighted sum of all pixels multiplied by a set of weights, one per output feature."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHdHZ-Na86hS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "87200be5-cec1-45c6-e1ee-8c10428da169"
      },
      "source": [
        "%matplotlib inline \n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import collections\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "torch.set_printoptions(edgeitems=2)\n",
        "torch.manual_seed(123)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7fb05a186850>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2heZVKv4Qw49",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class_names = ['airplane','automobile','bird','cat','deer',\n",
        "               'dog','frog','horse','ship','truck']"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "acwAHrZSQzkI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103,
          "referenced_widgets": [
            "d9a18bc06db84b15ace14005bee40684",
            "6c28d961c946488db408f487ba4583e9",
            "e96a24a9b21d40138e9429b51ff90913",
            "db6c57a072324042956dd2598b2b8180",
            "2019e4fc31ff4843ab4a8062a1da2aa6",
            "6d77d8c31ea8407a9d47dabe85852ecf",
            "cf65382f39824b9ba4a9ef190eaba3b4",
            "ef3fe1cf1fc546ba826a9c45a0443e10"
          ]
        },
        "outputId": "db6b80d1-a9ea-433e-8a82-51c2d91c194f"
      },
      "source": [
        "from torchvision import datasets, transforms\n",
        "\n",
        "data_path = '../data-unversioned/p1ch6/'\n",
        "cifar10 = datasets.CIFAR10(data_path, train=True, download=True,\n",
        "                           transform=transforms.Compose([\n",
        "                                                         transforms.ToTensor(),\n",
        "                                                         transforms.Normalize(\n",
        "                                                             (0.4915, 0.4823, 0.4468),\n",
        "                                                             (0.2470, 0.2435, 0.2616))\n",
        "                                                         ]))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ../data-unversioned/p1ch6/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d9a18bc06db84b15ace14005bee40684",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ../data-unversioned/p1ch6/cifar-10-python.tar.gz to ../data-unversioned/p1ch6/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cfcpy1TkROuR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cifar10_val = datasets.CIFAR10(data_path, train=False, download=False,\n",
        "                               transform=transforms.Compose([\n",
        "                              transforms.ToTensor(),\n",
        "                              transforms.Normalize((0.4915, 0.4823, 0.4468),\n",
        "                                                   (0.2470, 0.2435, 0.2616))\n",
        "                              ]))"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dMcmHmVXRc_v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "label_map = {0:1, 2:1}\n",
        "class_names = ['airplane', 'bird']\n",
        "cifar2 = [(img, label_map[label]) for img, label in cifar10 if label in [0, 2]]\n",
        "cifar2_val = [(img, label_map[label]) for img, label in cifar10_val if label in [0, 2]]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7qRa8T9D2S2",
        "colab_type": "text"
      },
      "source": [
        "# Convolutions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OV3bQ3GsO8S-",
        "colab_type": "text"
      },
      "source": [
        "`nn.Conv1d` for time series, `nn.Conv2d` for images and `nn.Conv3d` for volumes or videos.\n",
        "\n",
        "The arguments we provide to `nn.Conv2d` are the:\n",
        "1. The number of input features (or channels, since we are dealing with so-called multi-channel images, i.e. more than one value per pixel)\n",
        "2. The number of output features (arbitrary number, the more channels in the output image, the more the capacity of the network)\n",
        "3. The size of the kernel\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzaFtlFNPrNk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "54d85822-52c5-4902-d991-ff563e688ba3"
      },
      "source": [
        "conv = nn.Conv2d(3, 16, kernel_size=3)\n",
        "conv"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WqTcXjlrQEQF",
        "colab_type": "text"
      },
      "source": [
        "We expect the weight tensor to be sized `n_input_channels x 3 x 3 x n_output_channels`. The bias is just a constant value we add to each channel of the output image.\n",
        "\n",
        "\\* weights are initialized randomly\n",
        "\n",
        "**As usual we need to add the zero-th batch dimension with unsqueeze** if we want to call the `conv` module with one input image. `nn.Conv2d` expects `B x C x H x W`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AkrnnzkSPv_W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2b154339-1389-4b72-bfd3-b227511146d8"
      },
      "source": [
        "conv.weight.shape, conv.bias.shape"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([16, 3, 3, 3]), torch.Size([16]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kK6YCWphP4Ue",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d574a366-cf76-47ce-db70-a504a93c552b"
      },
      "source": [
        "img, _ = cifar2[0]\n",
        "output = conv(img.unsqueeze(0))\n",
        "img.unsqueeze(0).shape, output.shape"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 3, 32, 32]), torch.Size([1, 16, 30, 30]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0M-U7tZkSffd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "outputId": "cc4f7550-83ac-4a5a-e7a5-e68bfad58d4c"
      },
      "source": [
        "plt.imshow(output[0,0].detach(), cmap='gray')\n",
        "plt.show()\n",
        "# Bird after a random convolution treatment"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oa2SWeq4SrS_",
        "colab_type": "text"
      },
      "source": [
        "The output size is slightly less because it is a side effect of deciding what to do at the boundary of the image."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfFK_RdXTy0L",
        "colab_type": "text"
      },
      "source": [
        "To preserve image size, PyTorch gives us the possibility of padding the image, creating ghost pixels around the border that value zero as far as the convolution is concerned."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-Shp8w7UOuU",
        "colab_type": "text"
      },
      "source": [
        "Specifying `padding=1` when `kernel_size=3` means that now `i00` has an extra set of neighbours above and left, so that an output of the convolution can be computed even in the corner of our original image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TnxVl1lPUimx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bf8f977c-cfb0-4c64-97b9-0982feb6ba61"
      },
      "source": [
        "conv = nn.Conv2d(3, 1, kernel_size=3, padding=1)\n",
        "output = conv(img.unsqueeze(0))\n",
        "img.unsqueeze(0).shape, output.shape"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 3, 32, 32]), torch.Size([1, 1, 32, 32]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nsvuHQAXUub8",
        "colab_type": "text"
      },
      "source": [
        "\\* the size of `weight` and `bias` dont change whether padding is used or not."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tmD23APjU2yT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# play with convolution setting; weights by hand\n",
        "with torch.no_grad():\n",
        "  conv.bias.zero_() # zero out bias to remove any confounding factor\n",
        "\n",
        "with torch.no_grad():\n",
        "  conv.weight.fill_(1.0 / 9.0)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "STcDRDoGVWV9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "outputId": "c67510b3-6ab7-40b6-effe-c073beca8c3c"
      },
      "source": [
        "# See the results in the output image\n",
        "output = conv(img.unsqueeze(0))\n",
        "plt.imshow(output[0, 0].detach(), cmap='gray')\n",
        "plt.show()\n",
        "# bird, this time blurred thanks to a constant convolution kernel."
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxkqKjzpVy5M",
        "colab_type": "text"
      },
      "source": [
        "Every pixel of the output is the average of a neighborhood of the input, so output pixels of the output will be correlated and change more smoothly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WXt3K-sWZx4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We can try something different\n",
        "conv = nn.Conv2d(3, 1, kernel_size=3, padding=1)\n",
        "\n",
        "with torch.no_grad():\n",
        "  conv.weight[:] = torch.tensor([[-1.0, 0.0, 1.0],\n",
        "                                   [-1.0, 0.0, 1.0],\n",
        "                                   [-1.0, 0.0, 1.0]])\n",
        "  conv.bias.zero_()"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jAXDV5FmWogb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "outputId": "ff57c877-f47e-4265-fc92-14dad74c3237"
      },
      "source": [
        "output = conv(img.unsqueeze(0))\n",
        "plt.imshow(output[0, 0].detach(), cmap='gray')\n",
        "plt.show()\n",
        "# Vertical edges throughout our bird because of hand-crafted kernel"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JwIhMfXOW1kz",
        "colab_type": "text"
      },
      "source": [
        "## Depth and pooling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "huQCAKDIacBI",
        "colab_type": "text"
      },
      "source": [
        "If we wish to downsample our image by half, we'll want to use a size of 2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eIuTchTSeE8J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8ca89fb4-170f-4c3c-9ff8-591c80f31eb3"
      },
      "source": [
        "pool = nn.MaxPool2d(2)\n",
        "output = pool(img.unsqueeze(0))\n",
        "\n",
        "img.unsqueeze(0).shape, output.shape"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 3, 32, 32]), torch.Size([1, 3, 16, 16]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4VwbjBeeMLg",
        "colab_type": "text"
      },
      "source": [
        "Proceed to building our convolutional neural network for detecting birds and planes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3kmtKWieYys",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = nn.Sequential(\n",
        "    nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
        "    nn.Tanh(),\n",
        "    nn.MaxPool2d(2),\n",
        "    nn.Conv2d(16, 8, kernel_size=3, padding=1),\n",
        "    nn.Tanh(),\n",
        "    nn.MaxPool2d(2),\n",
        ")"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wx_374mvenbK",
        "colab_type": "text"
      },
      "source": [
        "Gone from 3 RGB channels to 16, thereby giving the network a chance to generate 16 independent features that will operate to hopefully discriminate low-level features of birds and airplanes. \n",
        "\n",
        "After the activation is applied, the 16-channel 32x32 image will be pooled to a 16-channel 16x16 image. \n",
        "\n",
        "At this point, the downsampled image will undergo another convolution that will generate a 8-channel 16x16 output, which again after the activation, will be pooled to a 8-channel 8x8 output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6MEpuJGgfSDB",
        "colab_type": "text"
      },
      "source": [
        "After the input image has been reduced to a set of 8x8 features, we expect some output of probabilities, so that we can feed our negative log likelihood. \n",
        "\n",
        "However, probabilities are a pair of numbers in a 1D vector (1 for bird, 1 for airplane, while we are still dealing with multi-channel 2D features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4hWy94u6fzYs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Turn our 8-channel 8x8 image into a 1D vector \n",
        "# Complete our network with a set of fully connected layers.\n",
        "model = nn.Sequential(\n",
        "    nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
        "    nn.Tanh(),\n",
        "    nn.MaxPool2d(2),\n",
        "    nn.Conv2d(16, 8, kernel_size=3, padding=1),\n",
        "    nn.Tanh(),\n",
        "    nn.MaxPool2d(2),\n",
        "    # ... Missing something important?\n",
        "    nn.Linear(8 * 8 * 8, 32),\n",
        "    nn.Tanh(),\n",
        "    nn.Linear(32, 2))\n",
        "# Linear layer is dependent on the expected size of the MaxPool2d output\n",
        "# i.e. 8 * 8 * 8 = 512"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "faiClP3OgT4V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3b234ac9-ceff-4e3d-a066-8f6e4d907634"
      },
      "source": [
        "numel_list = [p.numel() for p in model.parameters()]\n",
        "sum(numel_list), numel_list"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(18090, [432, 16, 1152, 8, 16384, 32, 64, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5x78-X5XgsbK",
        "colab_type": "text"
      },
      "source": [
        "Reasonable! In order to increase the capacity of the model, we could\n",
        "1. increase the number of output channels for the conv layers (which would lead to the linear layer increasing in size as well)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Di1QZ0sMhFl6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 327
        },
        "outputId": "53a3d263-1526-4e9d-e73b-ba5759b07742"
      },
      "source": [
        "model(img.unsqueeze(0))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-9c784fd7714c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1610\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1611\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1612\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1613\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1614\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: size mismatch, m1: [64 x 8], m2: [512 x 32] at /pytorch/aten/src/TH/generic/THTensorMath.cpp:41"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFx1p_YUhKpD",
        "colab_type": "text"
      },
      "source": [
        "What is missing there is the **reshaping step** from a 8-channel 8x8 image to a 512-element, 1D vector.\n",
        "\n",
        "We could use `view` on the output of the last `nn.MaxPool2d` but we dont have visibility of output of each module when we use `nn.Sequential`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iiW0_iJNhiet",
        "colab_type": "text"
      },
      "source": [
        "# Subclassing nn.Module"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTHF_tcEL2hq",
        "colab_type": "text"
      },
      "source": [
        "we need some flexibility therefore we need to subclass. In order to subclass, at the minimum we need to define the `forward` function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PeULIn1RMBcE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Net(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Net, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
        "    self.act1 = nn.Tanh()\n",
        "    self.pool1 = nn.MaxPool2d(2)\n",
        "    self.conv2 = nn.Conv2d(16, 8, kernel_size=3, padding=1)\n",
        "    self.act2 = nn.Tanh()\n",
        "    self.pool2 = nn.MaxPool2d(2)\n",
        "    self.fc1 = nn.Linear(8 * 8 * 8, 32)\n",
        "    self.act3 = nn.Tanh()\n",
        "    self.fc2 = nn.Linear(32, 2)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = self.pool1(self.act1(self.conv1(x)))\n",
        "    out = self.pool2(self.act2(self.conv2(out)))\n",
        "    out = self.view(-1, 8 * 8 * 8) # The reshape. we have been missing\n",
        "    out = self.act3(self.fc1(out))\n",
        "    out = self.fc2(out)\n",
        "    return out"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cx_UPHUKNN5y",
        "colab_type": "text"
      },
      "source": [
        "This `Net` class is equivalent to the `nn.Sequential` model we build in terms of submodules, by writing the `forward` function explicitly, we were able to manipulate the output of `self.pool3` directly and call `view` om it to turn it into a BxN vector. Note that we leave the batch dimension as -1 in the call to view, since in principle we don't know how many samples there'll be in the batch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNxds2SuN5ne",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Net()"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C17-szgZTCvF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c7aa586d-6283-4286-dcef-e1299e971b2e"
      },
      "source": [
        "numel_list = [p.numel() for p in model.parameters()]\n",
        "sum(numel_list), numel_list"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(18090, [432, 16, 1152, 8, 16384, 32, 64, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C55w2zCPTIZh",
        "colab_type": "text"
      },
      "source": [
        "It would appear as a waste to be registering submodules with no parameters such as `nn.Tanh` and `nn.MaxPool2d`. We can call those in the `forward` function. PyToch has functional counterparts for every `nn` module. \"Functional\" meaning no internal state or whose output value is solely and fully determined by the value input arguments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbAn9gb3XM7Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Net(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Net, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
        "    self.conv2 = nn.Conv2d(16, 8, kernel_size=3, padding=1)\n",
        "    self.fc1 = nn.Linear(8 * 8 * 8, 32)\n",
        "    self.fc2 = nn.Linear(32, 2)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
        "    out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
        "    out = out.view(-1, 8 * 8 * 8)\n",
        "    out = torch.tanh(self.fc1(out))\n",
        "    out = self.fc2(out)\n",
        "    return out"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8zGNk6QYMHO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d4578fdb-d6c1-49a2-ce70-aeefa04ef917"
      },
      "source": [
        "model = Net()\n",
        "numel_list = [p.numel() for p in model.parameters()]\n",
        "sum(numel_list), numel_list"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(18090, [432, 16, 1152, 8, 16384, 32, 64, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3GcwNlFXYpvw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e5a0d624-ee5f-4469-865a-2eee220202d8"
      },
      "source": [
        "# Double check our model runs\n",
        "model(img.unsqueeze(0))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.0234, 0.0825]], grad_fn=<AddmmBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50F0uxd4ZTAL",
        "colab_type": "text"
      },
      "source": [
        "We've got 2 numbers out! Information flows correctly. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YFbWBX9-Zu5s",
        "colab_type": "text"
      },
      "source": [
        "# Training our Convnet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6l-IBWNZyIe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Writing the training loop\n",
        "import datetime\n",
        "\n",
        "def training_loop(n_epochs, optimizer, model, loss_fn, train_loader):\n",
        "  for epoch in range(1, n_epochs + 1):\n",
        "    loss_train = 0.0\n",
        "    for imgs, labels in train_loader:\n",
        "      outputs = model(imgs)\n",
        "      loss = loss_fn(outputs, labels)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      loss_train += loss.item()\n",
        "\n",
        "    if epoch == 1 or epoch % 10 == 0:\n",
        "      print('{} Epoch {}, Training loss {}'.format(\n",
        "          datetime.datetime.now(), epoch, float(loss_train)))\n"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pid1DcqOalV8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "f5038152-8016-4ff9-9c91-41a3d8823f7b"
      },
      "source": [
        "# Train our model\n",
        "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64, shuffle=True)\n",
        "\n",
        "model = Net()\n",
        "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "training_loop(\n",
        "    n_epochs = 100,\n",
        "    optimizer = optimizer, \n",
        "    model = model,\n",
        "    loss_fn = loss_fn,\n",
        "    train_loader = train_loader,\n",
        ")"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-06-25 21:50:56.575635 Epoch 1, Training loss 20.153247013688087\n",
            "2020-06-25 21:51:23.489326 Epoch 10, Training loss 0.20436260174028575\n",
            "2020-06-25 21:51:53.067695 Epoch 20, Training loss 0.09466910781338811\n",
            "2020-06-25 21:52:23.560837 Epoch 30, Training loss 0.061047998431604356\n",
            "2020-06-25 21:52:53.961979 Epoch 40, Training loss 0.04488775183563121\n",
            "2020-06-25 21:53:24.210653 Epoch 50, Training loss 0.03542189617292024\n",
            "2020-06-25 21:53:54.667978 Epoch 60, Training loss 0.029206678955233656\n",
            "2020-06-25 21:54:24.913268 Epoch 70, Training loss 0.02482855212292634\n",
            "2020-06-25 21:54:55.132774 Epoch 80, Training loss 0.02158727651112713\n",
            "2020-06-25 21:55:25.467548 Epoch 90, Training loss 0.01907383670913987\n",
            "2020-06-25 21:55:55.618672 Epoch 100, Training loss 0.017078559605579358\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2PPVft6RczWE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "02d67245-60a0-4ccc-b1f1-d38330285028"
      },
      "source": [
        "# Look at accuracies on the training and val sets\n",
        "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64, shuffle=False)\n",
        "val_loader = torch.utils.data.DataLoader(cifar2_val, batch_size=64, shuffle=False)\n",
        "\n",
        "for loader in [train_loader, val_loader]:\n",
        "  correct = 0\n",
        "  total = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for imgs, labels in loader:\n",
        "      outputs = model(imgs)\n",
        "      _, predicted = torch.max(outputs, dim=1) # This will give us the index of the highest value in output\n",
        "      total += labels.shape[0]\n",
        "      correct += int((predicted == labels).sum())\n",
        "\n",
        "  print(\"Accuracy: %f\"% (correct / total))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 1.000000\n",
            "Accuracy: 1.000000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLd7letGgWu1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save(model.state_dict(), data_path + 'birds_vs_planes.pt')"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wLyt9PfhI7W",
        "colab_type": "text"
      },
      "source": [
        "The `birds_vs_planes.pt` file now contains all the parametes of the `model`, that is weights and biases for the two convolution modules and the two linear modules.\n",
        "\n",
        "So, no structure, just the weights. This means that when we deploy the model in production for our friend, we'll need to keep the model class handy, create an instance and then load parameters back into it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1ThCarkiMdo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0df44e99-45ba-412a-9f92-6df3a07b6d47"
      },
      "source": [
        "loaded_model = Net()\n",
        "loaded_model.load_state_dict(torch.load('./data-unversioned/p1ch6/birds_vs_planes.pt'))"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NHFzGKnJigra",
        "colab_type": "text"
      },
      "source": [
        "# Implementing fundamentals"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l521-PHo24ni",
        "colab_type": "text"
      },
      "source": [
        "## Width"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Myj2xAik29xG",
        "colab_type": "text"
      },
      "source": [
        "Width of the network, that is the number of neurons per layer, or channels per convolution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOsMzY0V3jYI",
        "colab_type": "text"
      },
      "source": [
        "We can make our model wider very easily."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_YZ7jtG3m0t",
        "colab_type": "text"
      },
      "source": [
        "**We just specify a larger number of output channels in the first convolution, and increase the subsequent layers accordingly**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZWt96Ipr3ysZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Net(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Net, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
        "    self.conv2 = nn.Conv2d(32, 16, kernel_size=3, padding=1)\n",
        "    self.fc1 = nn.Linear(8 * 8 * 16, 32)\n",
        "    self.fc2 = nn.Linear(32, 2)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = F.maxpool2d(torch.tanh(self.conv1(x)), 2)\n",
        "    out = F.maxpool2d(torch.tanh(self.conv2(out)),2)\n",
        "    out = out.view(-1, 8 * 8 * 16)\n",
        "    out = torch.tanh(self.fc1(out))\n",
        "    out = self.fc2(out)\n",
        "    return out"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgGBjuss4wOA",
        "colab_type": "text"
      },
      "source": [
        "If we want to avoid hard-coding numbers in the definition of the model, we can easily **pass a parameter to `init` and parameterize width, taking care to also parameterize the call to `view` in the forward function**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgJHiBEz5OP6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Net(nn.Module):\n",
        "  def __init__(self, n_chans1=32):\n",
        "    super(Net, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
        "    self.conv2 = nn.Conv2d(n_chans1, n_chans1 // 2, kernel_size=3, padding=1) # floor division\n",
        "    self.fc1 = nn.Linear(8 * 8 * n_chans1 // 2, 32)\n",
        "    self.fc2 = nn.Linear(32, 2)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
        "    out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
        "    out = out.view(-1, 8 * 8 * self.n_chans1 // 2)\n",
        "    out = torch.tanh(self.fc1(out))\n",
        "    out = self.fc2(out)\n",
        "    return out\n",
        "\n",
        "model = Net(n_chans1=32)\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NvL_T2ra6U_N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}