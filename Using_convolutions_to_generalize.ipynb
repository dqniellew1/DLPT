{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Using convolutions to generalize.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1CkVi73x_T9YI_RHVdh5aZJy-dYEn4C5c",
      "authorship_tag": "ABX9TyMygJoq4qkusFERRFBW3zxR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "082368674667424aa958264c43dfa8d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_1c1771f1eb514c14bc6e6596e4400396",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_5aa31fba20b84d33bd922aa59491a4e1",
              "IPY_MODEL_3df199d0938f443f95d5aebdccb2d9de"
            ]
          }
        },
        "1c1771f1eb514c14bc6e6596e4400396": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5aa31fba20b84d33bd922aa59491a4e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_b9787ca69d4f48d5be857f62b7f25ce4",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a9e51b9cb5734f1395a2e3285b01b225"
          }
        },
        "3df199d0938f443f95d5aebdccb2d9de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_59430c8d4a6649acb379c96718a5ee81",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 170500096/? [00:30&lt;00:00, 13436475.35it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7622390f97604a0caaffbe2a05844520"
          }
        },
        "b9787ca69d4f48d5be857f62b7f25ce4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a9e51b9cb5734f1395a2e3285b01b225": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "59430c8d4a6649acb379c96718a5ee81": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7622390f97604a0caaffbe2a05844520": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dqniellew1/DLPT/blob/master/Using_convolutions_to_generalize.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9T3pQNnL7Ud",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "drive_dir = 'drive/My Drive/dlwpt-code/data/'"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ph0WLgkvL-KU",
        "colab_type": "text"
      },
      "source": [
        "Taking a 1D view of our input image and multiplying it with a `n_output_features x n_input_features` weight matrix, as done in `nn.Linear`, means taking all pixels in the image, and for each channel computing a weighted sum of all pixels multiplied by a set of weights, one per output feature."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHdHZ-Na86hS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "75577bc5-fa67-4518-a7e7-9eca10627567"
      },
      "source": [
        "%matplotlib inline \n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import collections\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "torch.set_printoptions(edgeitems=2)\n",
        "torch.manual_seed(123)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7fa54d961990>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2heZVKv4Qw49",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class_names = ['airplane','automobile','bird','cat','deer',\n",
        "               'dog','frog','horse','ship','truck']"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "acwAHrZSQzkI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103,
          "referenced_widgets": [
            "082368674667424aa958264c43dfa8d9",
            "1c1771f1eb514c14bc6e6596e4400396",
            "5aa31fba20b84d33bd922aa59491a4e1",
            "3df199d0938f443f95d5aebdccb2d9de",
            "b9787ca69d4f48d5be857f62b7f25ce4",
            "a9e51b9cb5734f1395a2e3285b01b225",
            "59430c8d4a6649acb379c96718a5ee81",
            "7622390f97604a0caaffbe2a05844520"
          ]
        },
        "outputId": "65b0f7cc-3d30-4a21-ab07-d28433361b0d"
      },
      "source": [
        "from torchvision import datasets, transforms\n",
        "\n",
        "data_path = '../data-unversioned/p1ch6/'\n",
        "cifar10 = datasets.CIFAR10(data_path, train=True, download=True,\n",
        "                           transform=transforms.Compose([\n",
        "                                                         transforms.ToTensor(),\n",
        "                                                         transforms.Normalize(\n",
        "                                                             (0.4915, 0.4823, 0.4468),\n",
        "                                                             (0.2470, 0.2435, 0.2616))\n",
        "                                                         ]))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ../data-unversioned/p1ch6/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "082368674667424aa958264c43dfa8d9",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ../data-unversioned/p1ch6/cifar-10-python.tar.gz to ../data-unversioned/p1ch6/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cfcpy1TkROuR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1c2f5ae3-9a53-4570-c990-75cd2447e624"
      },
      "source": [
        "cifar10_val = datasets.CIFAR10(data_path, train=False, download=True,\n",
        "                               transform=transforms.Compose([\n",
        "                              transforms.ToTensor(),\n",
        "                              transforms.Normalize((0.4915, 0.4823, 0.4468),\n",
        "                                                   (0.2470, 0.2435, 0.2616))\n",
        "                              ]))"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dMcmHmVXRc_v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "label_map = {0:1, 2:1}\n",
        "class_names = ['airplane', 'bird']\n",
        "cifar2 = [(img, label_map[label]) for img, label in cifar10 if label in [0, 2]]\n",
        "cifar2_val = [(img, label_map[label]) for img, label in cifar10_val if label in [0, 2]]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7qRa8T9D2S2",
        "colab_type": "text"
      },
      "source": [
        "# Convolutions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OV3bQ3GsO8S-",
        "colab_type": "text"
      },
      "source": [
        "`nn.Conv1d` for time series, `nn.Conv2d` for images and `nn.Conv3d` for volumes or videos.\n",
        "\n",
        "The arguments we provide to `nn.Conv2d` are the:\n",
        "1. The number of input features (or channels, since we are dealing with so-called multi-channel images, i.e. more than one value per pixel)\n",
        "2. The number of output features (arbitrary number, the more channels in the output image, the more the capacity of the network)\n",
        "3. The size of the kernel\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzaFtlFNPrNk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6c87ec57-b619-4e5b-ff56-5ce59f27f31d"
      },
      "source": [
        "conv = nn.Conv2d(3, 16, kernel_size=3)\n",
        "conv"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WqTcXjlrQEQF",
        "colab_type": "text"
      },
      "source": [
        "We expect the weight tensor to be sized `n_input_channels x 3 x 3 x n_output_channels`. The bias is just a constant value we add to each channel of the output image.\n",
        "\n",
        "\\* weights are initialized randomly\n",
        "\n",
        "**As usual we need to add the zero-th batch dimension with unsqueeze** if we want to call the `conv` module with one input image. `nn.Conv2d` expects `B x C x H x W`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AkrnnzkSPv_W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8108eefa-74c0-44a0-d3e0-e8600f892fad"
      },
      "source": [
        "conv.weight.shape, conv.bias.shape"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([16, 3, 3, 3]), torch.Size([16]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kK6YCWphP4Ue",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e634dcd1-c936-45bf-8450-8b537be498d2"
      },
      "source": [
        "img, _ = cifar2[0]\n",
        "output = conv(img.unsqueeze(0))\n",
        "img.unsqueeze(0).shape, output.shape"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 3, 32, 32]), torch.Size([1, 16, 30, 30]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0M-U7tZkSffd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "outputId": "ce407027-b9bc-49a3-829a-a596e460565a"
      },
      "source": [
        "plt.imshow(output[0,0].detach(), cmap='gray')\n",
        "plt.show()\n",
        "# Bird after a random convolution treatment"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1XIzl9NPyZCy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        },
        "outputId": "d4bb475e-bf67-4670-f0d8-3b8947186f5b"
      },
      "source": [
        "plt.figure(figsize=(10, 4.8))\n",
        "ax1 = plt.subplot(1, 2, 1)\n",
        "plt.title(\"output\")\n",
        "plt.imshow(output[0, 0].detach(), cmap='gray')\n",
        "plt.subplot(1, 2, 2, sharex=ax1, sharey=ax1)\n",
        "plt.imshow(img.mean(0), cmap='gray')\n",
        "plt.title(\"input\")\n",
        "plt.show()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x345.6 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oa2SWeq4SrS_",
        "colab_type": "text"
      },
      "source": [
        "The output size is slightly less because it is a side effect of deciding what to do at the boundary of the image."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfFK_RdXTy0L",
        "colab_type": "text"
      },
      "source": [
        "To preserve image size, PyTorch gives us the possibility of padding the image, creating ghost pixels around the border that value zero as far as the convolution is concerned."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-Shp8w7UOuU",
        "colab_type": "text"
      },
      "source": [
        "Specifying `padding=1` when `kernel_size=3` means that now `i00` has an extra set of neighbours above and left, so that an output of the convolution can be computed even in the corner of our original image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TnxVl1lPUimx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bf8f977c-cfb0-4c64-97b9-0982feb6ba61"
      },
      "source": [
        "conv = nn.Conv2d(3, 1, kernel_size=3, padding=1)\n",
        "output = conv(img.unsqueeze(0))\n",
        "img.unsqueeze(0).shape, output.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 3, 32, 32]), torch.Size([1, 1, 32, 32]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nsvuHQAXUub8",
        "colab_type": "text"
      },
      "source": [
        "\\* the size of `weight` and `bias` dont change whether padding is used or not."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tmD23APjU2yT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# play with convolution setting; weights by hand\n",
        "with torch.no_grad():\n",
        "  conv.bias.zero_() # zero out bias to remove any confounding factor\n",
        "\n",
        "with torch.no_grad():\n",
        "  conv.weight.fill_(1.0 / 9.0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "STcDRDoGVWV9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "outputId": "c67510b3-6ab7-40b6-effe-c073beca8c3c"
      },
      "source": [
        "# See the results in the output image\n",
        "output = conv(img.unsqueeze(0))\n",
        "plt.imshow(output[0, 0].detach(), cmap='gray')\n",
        "plt.show()\n",
        "# bird, this time blurred thanks to a constant convolution kernel."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxkqKjzpVy5M",
        "colab_type": "text"
      },
      "source": [
        "Every pixel of the output is the average of a neighborhood of the input, so output pixels of the output will be correlated and change more smoothly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WXt3K-sWZx4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We can try something different\n",
        "conv = nn.Conv2d(3, 1, kernel_size=3, padding=1)\n",
        "\n",
        "with torch.no_grad():\n",
        "  conv.weight[:] = torch.tensor([[-1.0, 0.0, 1.0],\n",
        "                                   [-1.0, 0.0, 1.0],\n",
        "                                   [-1.0, 0.0, 1.0]])\n",
        "  conv.bias.zero_()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jAXDV5FmWogb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "outputId": "ff57c877-f47e-4265-fc92-14dad74c3237"
      },
      "source": [
        "output = conv(img.unsqueeze(0))\n",
        "plt.imshow(output[0, 0].detach(), cmap='gray')\n",
        "plt.show()\n",
        "# Vertical edges throughout our bird because of hand-crafted kernel"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JwIhMfXOW1kz",
        "colab_type": "text"
      },
      "source": [
        "## Depth and pooling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "huQCAKDIacBI",
        "colab_type": "text"
      },
      "source": [
        "If we wish to downsample our image by half, we'll want to use a size of 2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eIuTchTSeE8J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8ca89fb4-170f-4c3c-9ff8-591c80f31eb3"
      },
      "source": [
        "pool = nn.MaxPool2d(2)\n",
        "output = pool(img.unsqueeze(0))\n",
        "\n",
        "img.unsqueeze(0).shape, output.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 3, 32, 32]), torch.Size([1, 3, 16, 16]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4VwbjBeeMLg",
        "colab_type": "text"
      },
      "source": [
        "Proceed to building our convolutional neural network for detecting birds and planes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3kmtKWieYys",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = nn.Sequential(\n",
        "    nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
        "    nn.Tanh(),\n",
        "    nn.MaxPool2d(2),\n",
        "    nn.Conv2d(16, 8, kernel_size=3, padding=1),\n",
        "    nn.Tanh(),\n",
        "    nn.MaxPool2d(2),\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wx_374mvenbK",
        "colab_type": "text"
      },
      "source": [
        "Gone from 3 RGB channels to 16, thereby giving the network a chance to generate 16 independent features that will operate to hopefully discriminate low-level features of birds and airplanes. \n",
        "\n",
        "After the activation is applied, the 16-channel 32x32 image will be pooled to a 16-channel 16x16 image. \n",
        "\n",
        "At this point, the downsampled image will undergo another convolution that will generate a 8-channel 16x16 output, which again after the activation, will be pooled to a 8-channel 8x8 output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6MEpuJGgfSDB",
        "colab_type": "text"
      },
      "source": [
        "After the input image has been reduced to a set of 8x8 features, we expect some output of probabilities, so that we can feed our negative log likelihood. \n",
        "\n",
        "However, probabilities are a pair of numbers in a 1D vector (1 for bird, 1 for airplane, while we are still dealing with multi-channel 2D features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4hWy94u6fzYs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Turn our 8-channel 8x8 image into a 1D vector \n",
        "# Complete our network with a set of fully connected layers.\n",
        "model = nn.Sequential(\n",
        "    nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
        "    nn.Tanh(),\n",
        "    nn.MaxPool2d(2),\n",
        "    nn.Conv2d(16, 8, kernel_size=3, padding=1),\n",
        "    nn.Tanh(),\n",
        "    nn.MaxPool2d(2),\n",
        "    # ... Missing something important?\n",
        "    nn.Linear(8 * 8 * 8, 32),\n",
        "    nn.Tanh(),\n",
        "    nn.Linear(32, 2))\n",
        "# Linear layer is dependent on the expected size of the MaxPool2d output\n",
        "# i.e. 8 * 8 * 8 = 512"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "faiClP3OgT4V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3b234ac9-ceff-4e3d-a066-8f6e4d907634"
      },
      "source": [
        "numel_list = [p.numel() for p in model.parameters()]\n",
        "sum(numel_list), numel_list"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(18090, [432, 16, 1152, 8, 16384, 32, 64, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5x78-X5XgsbK",
        "colab_type": "text"
      },
      "source": [
        "Reasonable! In order to increase the capacity of the model, we could\n",
        "1. increase the number of output channels for the conv layers (which would lead to the linear layer increasing in size as well)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Di1QZ0sMhFl6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 327
        },
        "outputId": "53a3d263-1526-4e9d-e73b-ba5759b07742"
      },
      "source": [
        "model(img.unsqueeze(0))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-9c784fd7714c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1610\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1611\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1612\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1613\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1614\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: size mismatch, m1: [64 x 8], m2: [512 x 32] at /pytorch/aten/src/TH/generic/THTensorMath.cpp:41"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFx1p_YUhKpD",
        "colab_type": "text"
      },
      "source": [
        "What is missing there is the **reshaping step** from a 8-channel 8x8 image to a 512-element, 1D vector.\n",
        "\n",
        "We could use `view` on the output of the last `nn.MaxPool2d` but we dont have visibility of output of each module when we use `nn.Sequential`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iiW0_iJNhiet",
        "colab_type": "text"
      },
      "source": [
        "# Subclassing nn.Module"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTHF_tcEL2hq",
        "colab_type": "text"
      },
      "source": [
        "we need some flexibility therefore we need to subclass. In order to subclass, at the minimum we need to define the `forward` function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PeULIn1RMBcE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Net(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Net, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
        "    self.act1 = nn.Tanh()\n",
        "    self.pool1 = nn.MaxPool2d(2)\n",
        "    self.conv2 = nn.Conv2d(16, 8, kernel_size=3, padding=1)\n",
        "    self.act2 = nn.Tanh()\n",
        "    self.pool2 = nn.MaxPool2d(2)\n",
        "    self.fc1 = nn.Linear(8 * 8 * 8, 32)\n",
        "    self.act3 = nn.Tanh()\n",
        "    self.fc2 = nn.Linear(32, 2)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = self.pool1(self.act1(self.conv1(x)))\n",
        "    out = self.pool2(self.act2(self.conv2(out)))\n",
        "    out = self.view(-1, 8 * 8 * 8) # The reshape. we have been missing\n",
        "    out = self.act3(self.fc1(out))\n",
        "    out = self.fc2(out)\n",
        "    return out"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cx_UPHUKNN5y",
        "colab_type": "text"
      },
      "source": [
        "This `Net` class is equivalent to the `nn.Sequential` model we build in terms of submodules, by writing the `forward` function explicitly, we were able to manipulate the output of `self.pool3` directly and call `view` om it to turn it into a BxN vector. Note that we leave the batch dimension as -1 in the call to view, since in principle we don't know how many samples there'll be in the batch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNxds2SuN5ne",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Net()"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C17-szgZTCvF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3ce3666f-4189-43ff-83af-95aba20a3024"
      },
      "source": [
        "numel_list = [p.numel() for p in model.parameters()]\n",
        "sum(numel_list), numel_list"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(18090, [432, 16, 1152, 8, 16384, 32, 64, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C55w2zCPTIZh",
        "colab_type": "text"
      },
      "source": [
        "It would appear as a waste to be registering submodules with no parameters such as `nn.Tanh` and `nn.MaxPool2d`. We can call those in the `forward` function. PyToch has functional counterparts for every `nn` module. \"Functional\" meaning no internal state or whose output value is solely and fully determined by the value input arguments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbAn9gb3XM7Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Net(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Net, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
        "    self.conv2 = nn.Conv2d(16, 8, kernel_size=3, padding=1)\n",
        "    self.fc1 = nn.Linear(8 * 8 * 8, 32)\n",
        "    self.fc2 = nn.Linear(32, 2)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
        "    out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
        "    out = out.view(-1, 8 * 8 * 8)\n",
        "    out = torch.tanh(self.fc1(out))\n",
        "    out = self.fc2(out)\n",
        "    return out"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8zGNk6QYMHO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9628a693-2df0-4df6-ef76-8240c38e01d5"
      },
      "source": [
        "model = Net()\n",
        "numel_list = [p.numel() for p in model.parameters()]\n",
        "sum(numel_list), numel_list"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(18090, [432, 16, 1152, 8, 16384, 32, 64, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3GcwNlFXYpvw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3f3f9184-db72-4504-bc1d-309eb446918d"
      },
      "source": [
        "# Double check our model runs\n",
        "model(img.unsqueeze(0))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.1176, -0.1158]], grad_fn=<AddmmBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50F0uxd4ZTAL",
        "colab_type": "text"
      },
      "source": [
        "We've got 2 numbers out! Information flows correctly. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YFbWBX9-Zu5s",
        "colab_type": "text"
      },
      "source": [
        "# Training our Convnet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LhIz5IJOzsR6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "df17ba44-ba53-43f7-ef7e-dea7aee0b5a7"
      },
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "print(f\"Training on device {device}.\")"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training on device cuda.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6l-IBWNZyIe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Writing the training loop\n",
        "import datetime\n",
        "\n",
        "def training_loop(n_epochs, optimizer, model, loss_fn, train_loader):\n",
        "  for epoch in range(1, n_epochs + 1):\n",
        "    loss_train = 0.0\n",
        "    for imgs, labels in train_loader:\n",
        "      imgs = imgs.to(device=device)\n",
        "      labels = labels.to(device=device)\n",
        "      outputs = model(imgs)\n",
        "      loss = loss_fn(outputs, labels)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      loss_train += loss.item()\n",
        "\n",
        "    if epoch == 1 or epoch % 10 == 0:\n",
        "      print('{} Epoch {}, Training loss {}'.format(\n",
        "          datetime.datetime.now(), epoch, float(loss_train)))\n"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pid1DcqOalV8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "88fed928-532d-4069-bfc8-7de869c2e080"
      },
      "source": [
        "# Train our model\n",
        "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64, shuffle=True)\n",
        "\n",
        "model = Net().to(device=device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "training_loop(\n",
        "    n_epochs = 100,\n",
        "    optimizer = optimizer, \n",
        "    model = model,\n",
        "    loss_fn = loss_fn,\n",
        "    train_loader = train_loader,\n",
        ")"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-06-27 03:37:41.647184 Epoch 1, Training loss 21.67556913383305\n",
            "2020-06-27 03:37:44.675656 Epoch 10, Training loss 0.2099430188536644\n",
            "2020-06-27 03:37:47.987282 Epoch 20, Training loss 0.09772227704524994\n",
            "2020-06-27 03:37:51.349432 Epoch 30, Training loss 0.06313114240765572\n",
            "2020-06-27 03:37:54.740249 Epoch 40, Training loss 0.046441905200481415\n",
            "2020-06-27 03:37:58.058597 Epoch 50, Training loss 0.03664662316441536\n",
            "2020-06-27 03:38:01.399238 Epoch 60, Training loss 0.030214853584766388\n",
            "2020-06-27 03:38:04.748245 Epoch 70, Training loss 0.025674592703580856\n",
            "2020-06-27 03:38:08.090368 Epoch 80, Training loss 0.022296961396932602\n",
            "2020-06-27 03:38:11.487913 Epoch 90, Training loss 0.019705064594745636\n",
            "2020-06-27 03:38:14.809684 Epoch 100, Training loss 0.017630696296691895\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2PPVft6RczWE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Look at accuracies on the training and val sets\n",
        "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64, shuffle=False)\n",
        "val_loader = torch.utils.data.DataLoader(cifar2_val, batch_size=64, shuffle=False)\n",
        "all_acc_dict = collections.OrderedDict()\n",
        "\n",
        "def validate(model, training_loader, val_loader):\n",
        "  accdict = {}\n",
        "  for name, loader in [(\"train\", train_loader), (\"val\", val_loader)]:\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for imgs, labels in loader:\n",
        "        imgs = imgs.to(device=device)\n",
        "        labels = labels.to(device=device)\n",
        "        outputs = model(imgs)\n",
        "        _, predicted = torch.max(outputs, dim=1) # This will give us the index of the highest value in output\n",
        "        total += labels.shape[0]\n",
        "        correct += int((predicted == labels).sum())\n",
        "    print(\"Accuracy {}: {:.2f}\".format(name, correct / total))\n",
        "    accdict[name] = correct / total\n",
        "  return accdict"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eOlccOpF2uTS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "bf52b6ca-fdd6-486a-c519-14164bc30a9f"
      },
      "source": [
        "all_acc_dict[\"baseline\"] = validate(model, train_loader, val_loader)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy train: 1.00\n",
            "Accuracy val: 1.00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLd7letGgWu1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save(model.state_dict(), data_path + 'birds_vs_planes.pt')"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wLyt9PfhI7W",
        "colab_type": "text"
      },
      "source": [
        "The `birds_vs_planes.pt` file now contains all the parametes of the `model`, that is weights and biases for the two convolution modules and the two linear modules.\n",
        "\n",
        "So, no structure, just the weights. This means that when we deploy the model in production for our friend, we'll need to keep the model class handy, create an instance and then load parameters back into it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1ThCarkiMdo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2d7cb7d6-9943-4aa1-ba76-21eb5269e904"
      },
      "source": [
        "loaded_model = Net().to(device=device)\n",
        "loaded_model.load_state_dict(torch.load('../data-unversioned/p1ch6/birds_vs_planes.pt', map_location=device))"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4MSCUwG01oqx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64, shuffle=True)\n",
        "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64, shuffle=False)\n",
        "val_loader = torch.utils.data.DataLoader(cifar2_val, batch_size=64, shuffle=False)"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NHFzGKnJigra",
        "colab_type": "text"
      },
      "source": [
        "# Implementing fundamentals"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l521-PHo24ni",
        "colab_type": "text"
      },
      "source": [
        "## Width"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Myj2xAik29xG",
        "colab_type": "text"
      },
      "source": [
        "Width of the network, that is the number of neurons per layer, or channels per convolution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOsMzY0V3jYI",
        "colab_type": "text"
      },
      "source": [
        "We can make our model wider very easily."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_YZ7jtG3m0t",
        "colab_type": "text"
      },
      "source": [
        "**We just specify a larger number of output channels in the first convolution, and increase the subsequent layers accordingly**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZWt96Ipr3ysZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NetWidth(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(NetWidth, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
        "    self.conv2 = nn.Conv2d(32, 16, kernel_size=3, padding=1)\n",
        "    self.fc1 = nn.Linear(8 * 8 * 16, 32)\n",
        "    self.fc2 = nn.Linear(32, 2)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
        "    out = F.max_pool2d(torch.tanh(self.conv2(out)),2)\n",
        "    out = out.view(-1, 8 * 8 * 16)\n",
        "    out = torch.tanh(self.fc1(out))\n",
        "    out = self.fc2(out)\n",
        "    return out"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PWX8PnJa1OrH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "25d63945-c1ef-45ed-9e53-b32c951bf538"
      },
      "source": [
        "model = NetWidth().to(device=device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "training_loop(\n",
        "    n_epochs = 100,\n",
        "    optimizer = optimizer,\n",
        "    model = model,\n",
        "    loss_fn = loss_fn,\n",
        "    train_loader = train_loader,\n",
        ")\n",
        "\n",
        "validate(model, train_loader, val_loader)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-06-27 03:46:18.740792 Epoch 1, Training loss 12.701901206746697\n",
            "2020-06-27 03:46:22.185796 Epoch 10, Training loss 0.17487512528896332\n",
            "2020-06-27 03:46:26.003539 Epoch 20, Training loss 0.08373378589749336\n",
            "2020-06-27 03:46:29.844016 Epoch 30, Training loss 0.05496018007397652\n",
            "2020-06-27 03:46:33.711698 Epoch 40, Training loss 0.04087359830737114\n",
            "2020-06-27 03:46:37.570852 Epoch 50, Training loss 0.0325222872197628\n",
            "2020-06-27 03:46:41.418088 Epoch 60, Training loss 0.026998203247785568\n",
            "2020-06-27 03:46:45.268896 Epoch 70, Training loss 0.02307383343577385\n",
            "2020-06-27 03:46:49.177256 Epoch 80, Training loss 0.02014296129345894\n",
            "2020-06-27 03:46:53.060943 Epoch 90, Training loss 0.01787097379565239\n",
            "2020-06-27 03:46:56.897364 Epoch 100, Training loss 0.01605827361345291\n",
            "Accuracy train: 1.00\n",
            "Accuracy val: 1.00\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'train': 1.0, 'val': 1.0}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgGBjuss4wOA",
        "colab_type": "text"
      },
      "source": [
        "If we want to avoid hard-coding numbers in the definition of the model, we can easily **pass a parameter to `init` and parameterize width, taking care to also parameterize the call to `view` in the forward function**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgJHiBEz5OP6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NetWidth(nn.Module):\n",
        "  def __init__(self, n_chans1=32):\n",
        "    super().__init__()\n",
        "    self.n_chans1 = n_chans1\n",
        "    self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
        "    self.conv2 = nn.Conv2d(n_chans1, n_chans1 // 2, kernel_size=3, padding=1) # floor division\n",
        "    self.fc1 = nn.Linear(8 * 8 * n_chans1 // 2, 32)\n",
        "    self.fc2 = nn.Linear(32, 2)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
        "    out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
        "    out = out.view(-1, 8 * 8 * self.n_chans1 // 2)\n",
        "    out = torch.tanh(self.fc1(out))\n",
        "    out = self.fc2(out)\n",
        "    return out\n",
        "\n",
        "model = NetWidth(n_chans1=32)\n"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3l-Avns-3rFK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "4bf2988b-7496-4c0e-d3b3-26c933fd0559"
      },
      "source": [
        "model = NetWidth(n_chans1=32).to(device=device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "training_loop(\n",
        "    n_epochs = 11,\n",
        "    optimizer = optimizer,\n",
        "    model = model,\n",
        "    loss_fn = loss_fn,\n",
        "    train_loader = train_loader,\n",
        ")\n",
        "\n",
        "all_acc_dict['width'] = validate(model, train_loader, val_loader)\n"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-06-27 03:47:29.634067 Epoch 1, Training loss 15.045918611809611\n",
            "2020-06-27 03:47:33.066622 Epoch 10, Training loss 0.20126965828239918\n",
            "Accuracy train: 1.00\n",
            "Accuracy val: 1.00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NvL_T2ra6U_N",
        "colab_type": "text"
      },
      "source": [
        "The numbers specifying channels and features for each layer are directly related to the number of parameters in a model, they increase the capacity of the model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ig-PpCZSishj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b9cdf44c-4416-4dab-d14f-1985087df0b1"
      },
      "source": [
        "sum([p.numel() for p in model.parameters()])"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "38386"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDXsWLpRiwnG",
        "colab_type": "text"
      },
      "source": [
        "The greater the capacity, the more **variability** in the inputs the model will be able to manage, but at the same time the more likely **overfitting** will be, since the model can leverage a greater number of paramters to memorize unessential aspects of the input."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDxgA6ySjKaE",
        "colab_type": "text"
      },
      "source": [
        "Ways to combat overfitting:\n",
        "1. Increase sample size\n",
        "2. Augment existing data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OgQYclN6jS_T",
        "colab_type": "text"
      },
      "source": [
        "Few more tricks:\n",
        "\n",
        "### **Regularization** \n",
        "Stabilize training by **adding a regularization term to the loss**. It is a penalty on larger values of the weights, so that model weights tend to be small and bounded.\n",
        "\n",
        "This makes the loss smoother. The most popular regularization terms is L2 which is the sum of squares of all weights in the model. and L1 ( sum of the absolute values of all weights in the model. **Both of them are scaled by a (small) factor, which is a hyperparameter we set before training.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCi4_l0Gj9RP",
        "colab_type": "text"
      },
      "source": [
        "L2 regularization is also called weight decay.\n",
        "\n",
        "Adding L2 regularization to the loss functions is equivalent to decreasing each weight by an amount proportional to **its current value** during the optimization step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P386jmoSksSu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Implement regularization in the training loop\n",
        "def training_loop_l2reg(n_epochs, optimizer, model, loss_fn, train_loader):\n",
        "  for epoch in range(1, n_epochs + 1):\n",
        "    loss_train = 0.0\n",
        "    for imgs, labels in train_loader:\n",
        "      imgs = imgs.to(device=device)\n",
        "      labels = labels.to(device=device)\n",
        "      outputs = model(imgs)\n",
        "      loss = loss_fn(outputs, labels)\n",
        "\n",
        "      l2_lambda = 0.0\n",
        "      l2_norm = sum(p.pow(2.0).sum() for p in model.parameters())\n",
        "      loss = loss + l2_lambda * l2_norm\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      loss_train += loss.item()\n",
        "    if epoch == 1 or epoch % 10 == 0:\n",
        "      print('{} Epoch {}, Training loss {}'.format(\n",
        "          datetime.datetime.now(), epoch, loss_train/len(train_loader)))\n",
        "\n",
        "# replace pow(2.0) with abs() for L1 regularization"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "la8XlITn5PRK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "d359c015-87e6-43dc-9224-9d83c6354f25"
      },
      "source": [
        "model = Net().to(device=device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "training_loop_l2reg(\n",
        "    n_epochs = 100,\n",
        "    optimizer = optimizer,\n",
        "    model = model,\n",
        "    loss_fn = loss_fn,\n",
        "    train_loader = train_loader,\n",
        ")\n",
        "\n",
        "all_acc_dict[\"l2_reg\"] = validate(model, train_loader, val_loader)"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-06-27 03:51:23.528419 Epoch 1, Training loss 0.08115141928955248\n",
            "2020-06-27 03:51:27.969339 Epoch 10, Training loss 0.0010817234588276809\n",
            "2020-06-27 03:51:32.900052 Epoch 20, Training loss 0.0005229609027789657\n",
            "2020-06-27 03:51:37.857072 Epoch 30, Training loss 0.000345058834097188\n",
            "2020-06-27 03:51:42.749320 Epoch 40, Training loss 0.00025744724330628756\n",
            "2020-06-27 03:51:47.656280 Epoch 50, Training loss 0.00020528107786634166\n",
            "2020-06-27 03:51:52.565983 Epoch 60, Training loss 0.00017066359235222932\n",
            "2020-06-27 03:51:57.446189 Epoch 70, Training loss 0.00014601142448224838\n",
            "2020-06-27 03:52:02.367739 Epoch 80, Training loss 0.00012756226833458918\n",
            "2020-06-27 03:52:07.288771 Epoch 90, Training loss 0.0001132382661294026\n",
            "2020-06-27 03:52:12.373885 Epoch 100, Training loss 0.00010179201508783231\n",
            "Accuracy train: 1.00\n",
            "Accuracy val: 1.00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPRSRlj7mgdP",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "The `SGD` optimizer in PyTorch already has `weight_decay` parameter."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oG0t6Q6smnpV",
        "colab_type": "text"
      },
      "source": [
        "### **Dropout**\n",
        "\n",
        "Zero out a random fraction of outputs from neurons across the network, where the randomization happens at each training iteration."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXfLRttYm5yt",
        "colab_type": "text"
      },
      "source": [
        "This procedure effectively generates slightly different models with different neurons topologies at each iteration, giving less chance to neurons in the model to coordinate in the memorization process happening during overfitting.\n",
        "\n",
        "Another point of view is that dropout perturbs the features being generated by the model, exerting an effect that is close to augmentation, but this time throughout the network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-VHGLi0njkN",
        "colab_type": "text"
      },
      "source": [
        "We can implement dropout in a model by adding a `nn.Dropout` module betwen the non-linear activation function and the linear or convolutional module of the subsqquent layer.\n",
        "\n",
        "As an argument, we need to specify the probability with which inputs will be zeroes out. In case of convolutions we'll use the specialized `nn.Dropout2D` or `nn.Dropout3D`, which zero out entire channels of the input."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NpWjBEhyoDeq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Implement dropout\n",
        "class NetDropout(nn.Module):\n",
        "  def __init__(self, n_chans1=32):\n",
        "    super().__init__()\n",
        "    self.n_chans1 = n_chans1\n",
        "    self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
        "    self.conv1_dropout = nn.Dropout2d(p=0.4)\n",
        "    self.conv2 = nn.Conv2d(n_chans1, n_chans1 // 2, kernel_size=3, padding=1)\n",
        "    self.conv2_dropout = nn.Dropout2d(p=0.4)\n",
        "    self.fc1 = nn.Linear(8 * 8 * n_chans1 // 2, 32)\n",
        "    self.fc2 = nn.Linear(32, 2)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
        "    out = self.conv1_dropout(out)\n",
        "    out = F.max_pool2d(torch.tanh(self.conv2(out)),2)\n",
        "    out = self.conv2_dropout(out)\n",
        "    out = out.view(-1, 8 * 8 * self.n_chans1 // 2)\n",
        "    out = torch.tanh(self.fc1(out))\n",
        "    out = self.fc2(out)\n",
        "    return out\n"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1N8dgIKpXRo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "996eb950-7077-43e9-d036-94a4bb94f220"
      },
      "source": [
        "model = NetDropout(n_chans1=32).to(device=device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "training_loop(\n",
        "    n_epochs = 100,\n",
        "    optimizer = optimizer,\n",
        "    model = model,\n",
        "    loss_fn = loss_fn,\n",
        "    train_loader = train_loader,\n",
        ")\n",
        "all_acc_dict[\"dropout\"] = validate(model, train_loader, val_loader)"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-06-27 03:53:25.618074 Epoch 1, Training loss 17.864584689959884\n",
            "2020-06-27 03:53:29.312050 Epoch 10, Training loss 0.2484072456136346\n",
            "2020-06-27 03:53:33.450595 Epoch 20, Training loss 0.10482838749885559\n",
            "2020-06-27 03:53:37.577204 Epoch 30, Training loss 0.07348472252488136\n",
            "2020-06-27 03:53:41.682066 Epoch 40, Training loss 0.05325973266735673\n",
            "2020-06-27 03:53:45.793159 Epoch 50, Training loss 0.043056090362370014\n",
            "2020-06-27 03:53:49.855466 Epoch 60, Training loss 0.04115982260555029\n",
            "2020-06-27 03:53:53.940124 Epoch 70, Training loss 0.028152160346508026\n",
            "2020-06-27 03:53:58.027598 Epoch 80, Training loss 0.02450842224061489\n",
            "2020-06-27 03:54:02.090534 Epoch 90, Training loss 0.06390692293643951\n",
            "2020-06-27 03:54:06.151157 Epoch 100, Training loss 0.06583886407315731\n",
            "Accuracy train: 1.00\n",
            "Accuracy val: 1.00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0bnntCop29W",
        "colab_type": "text"
      },
      "source": [
        "Note that dropout is normally active during training, **while during the evaluation of a trained model in production dropout is bypassed, or equivalently, it is assigned a probability equal to 0.**\n",
        "\n",
        "This is controlled through the `train` property of the `Dropout` module. We recall that PyTorch lets us switch between the two modalities by calling:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SwKJex8AphUn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "1ba67897-b8a3-4ac1-b007-ae410b336711"
      },
      "source": [
        "model.train()"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NetDropout(\n",
              "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (conv1_dropout): Dropout2d(p=0.4, inplace=False)\n",
              "  (conv2): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (conv2_dropout): Dropout2d(p=0.4, inplace=False)\n",
              "  (fc1): Linear(in_features=1024, out_features=32, bias=True)\n",
              "  (fc2): Linear(in_features=32, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRRML3H8pyeI",
        "colab_type": "text"
      },
      "source": [
        "or"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "404rSsZ_qaLG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "195df488-83d8-4bda-c9d9-53f649a3e745"
      },
      "source": [
        "model.eval()"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NetDropout(\n",
              "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (conv1_dropout): Dropout2d(p=0.4, inplace=False)\n",
              "  (conv2): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (conv2_dropout): Dropout2d(p=0.4, inplace=False)\n",
              "  (fc1): Linear(in_features=1024, out_features=32, bias=True)\n",
              "  (fc2): Linear(in_features=32, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONXbKpD9qbAy",
        "colab_type": "text"
      },
      "source": [
        "### **Batch Normalization**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wDAj89GsS94",
        "colab_type": "text"
      },
      "source": [
        "Alternative to dropout. Allows the increase of learning rate, making the training less dependent on initialization and act as a regularizer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDQvfZMDsjya",
        "colab_type": "text"
      },
      "source": [
        "Idea behind it: **Rescale the inputs to the activations of the network so that minibatches have a certain desireable distribution.** Recalling the mechanic of learning and the role of non-linear activation functions, this helps avoiding that the inputs to activations functions are too far into in the saturateed portion of the function, thereby killing gradients and slowing training altogether."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u32mQSTgtAaV",
        "colab_type": "text"
      },
      "source": [
        "In practical terms, **batch norm shifts and scales an intermediate input using the mean and standard deviation collected at the intermediate location over the samples of the minibatch.** The regularization effect is a result of the fact that an individual sample and its downstream activations are always seen by the model shifted and scaled depending on the statistics across the randomly extracted minibatch. This is in itself a form of principled augmentation. The paper suggests that when using batch normalization eliminates or at least alleviates the need for dropout."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SlJa5nHZtxO3",
        "colab_type": "text"
      },
      "source": [
        "Batch norm in PyTorch is provided through the `nn.BatchNorm1d, 2d or 3d` modules according to the **dimensionality of the input**. Since the aim for batch normalization is to rescale the inputs of the activations, the natural location is after the linear transformation (convolution in this case) and the activation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NK7xlvsruI7A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Batch norm implementation\n",
        "class NetBatchNorm(nn.Module):\n",
        "  def __init__(self, n_chans1=32):\n",
        "    super().__init__()\n",
        "    self.n_chans1 = n_chans1\n",
        "    self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
        "    self.conv1_batchnorm = nn.BatchNorm2d(num_features=n_chans1)\n",
        "    self.conv2 = nn.Conv2d(n_chans1, n_chans1 // 2, kernel_size=3, padding=1)\n",
        "    self.conv2_batchnorm = nn.BatchNorm2d(num_features= n_chans1 // 2)\n",
        "    self.fc1 = nn.Linear(8 * 8 * n_chans1 // 2, 32)\n",
        "    self.fc2 = nn.Linear(32, 2)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = self.conv1_batchnorm(self.conv1(x))\n",
        "    out = F.max_pool2d(torch.tanh(out), 2)\n",
        "    out = self.conv2_batchnorm(self.conv2(out))\n",
        "    out = F.max_pool2d(torch.tanh(out), 2)\n",
        "    out = out.view(-1, 8 * 8 * self.n_chans1 // 2)\n",
        "    out = torch.tanh(self.fc1(out))\n",
        "    out = self.fc2(out)\n",
        "    return out"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2J-m4yBXv1bo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "31fe9156-5981-4b38-ac52-c40ac607e648"
      },
      "source": [
        "model = NetBatchNorm(n_chans1=32).to(device=device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "training_loop(\n",
        "    n_epochs = 100,\n",
        "    optimizer = optimizer, \n",
        "    model = model,\n",
        "    loss_fn = loss_fn,\n",
        "    train_loader = train_loader,\n",
        "    )\n",
        "\n",
        "all_acc_dict[\"batch_norm\"] = validate(model, train_loader, val_loader)"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-06-27 03:55:11.075565 Epoch 1, Training loss 7.033205991610885\n",
            "2020-06-27 03:55:15.201790 Epoch 10, Training loss 0.15597037598490715\n",
            "2020-06-27 03:55:19.776893 Epoch 20, Training loss 0.077224962413311\n",
            "2020-06-27 03:55:24.368912 Epoch 30, Training loss 0.05141325294971466\n",
            "2020-06-27 03:55:28.984839 Epoch 40, Training loss 0.038555216044187546\n",
            "2020-06-27 03:55:33.588500 Epoch 50, Training loss 0.030849691480398178\n",
            "2020-06-27 03:55:38.183881 Epoch 60, Training loss 0.025714285671710968\n",
            "2020-06-27 03:55:42.784551 Epoch 70, Training loss 0.022045768797397614\n",
            "2020-06-27 03:55:47.408171 Epoch 80, Training loss 0.019293613731861115\n",
            "2020-06-27 03:55:52.018059 Epoch 90, Training loss 0.01715223863720894\n",
            "2020-06-27 03:55:56.614916 Epoch 100, Training loss 0.01543845608830452\n",
            "Accuracy train: 1.00\n",
            "Accuracy val: 1.00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87eGgUTp6ZPo",
        "colab_type": "text"
      },
      "source": [
        "Just like dropout, batch norm needs to behave differently during training and inference. In fact, at inference time we want to avoid the output for a specific input to depend on the statistics of the other inputs we're presenting to the model. As such, we need a way to still normalize, but this time ficing the normalization parameters once and for all."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QviMxzyW7VnS",
        "colab_type": "text"
      },
      "source": [
        "As batches are processed, in addition to estimating mean and standard deviation for the current minibatch, PyTorch also updates running estimates for mean and standard deviation that is representative of the whole dataset, as an approximation. This way when the use specifies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_xXOVWD7oQF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "b164680f-9893-4ff9-bd65-047ab0320014"
      },
      "source": [
        "model.eval()"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NetBatchNorm(\n",
              "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (conv1_batchnorm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (conv2): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (conv2_batchnorm): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (fc1): Linear(in_features=1024, out_features=32, bias=True)\n",
              "  (fc2): Linear(in_features=32, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LzBfE2hQ7pEf",
        "colab_type": "text"
      },
      "source": [
        "and the model contains a batch normalization module, the running estimates are frozen and used for normalization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOlwa0DX7wAB",
        "colab_type": "text"
      },
      "source": [
        "### **Depth**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MdjHhgpR71I4",
        "colab_type": "text"
      },
      "source": [
        "Deeper models are always better than shallow ones, arent they? It depends. With depth, the complexity of the function the network is able to approximate generally increases. Depth allows a model to deal with hierachical information, where we need to understand context in order to say something about some input."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5kfxUkx8Usu",
        "colab_type": "text"
      },
      "source": [
        "Another way to think about depth is increasing depth is related to increasing the length of the sequence of operations that the network will be able to perform when processing some input. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TvtkBhh8ucZ",
        "colab_type": "text"
      },
      "source": [
        "**Skip connections**\n",
        "\n",
        "Adding depth to a model generally makes training harder to converge."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "moijpD__9Q6H",
        "colab_type": "text"
      },
      "source": [
        "A skip connection is nothing but an addition of the input to the output of a block of layers. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hdkeR6GF9qJs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Lets add one layer to our simple convolutional model and use ReLU for activation\n",
        "class DepthNet(nn.Module):\n",
        "  def __init__(self, n_chans1=32):\n",
        "    super().__init__()\n",
        "    self.n_chans1 = n_chans1\n",
        "    self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
        "    self.conv2 = nn.Conv2d(n_chans1, n_chans1 // 2, kernel_size=3, padding=1)\n",
        "    self.conv3 = nn.Conv2d(n_chans1 // 2, n_chans1 // 2, kernel_size=3, padding=1)\n",
        "    self.fc1 = nn.Linear(4 * 4 * n_chans1 // 2, 32)\n",
        "    self.fc2 = nn.Linear(32, 2)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = F.max_pool2d(torch.relu(self.conv1(x)), 2)\n",
        "    out = F.max_pool2d(torch.relu(self.conv2(out)), 2)\n",
        "    out = F.max_pool2d(torch.relu(self.conv3(out)), 2)\n",
        "    out = out.view(-1, 4 * 4 * self.n_chans1 // 2)\n",
        "    out = torch.relu(self.fc1(out))\n",
        "    out = self.fc2(out)\n",
        "    return out"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ar8gdorK-4yG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "f47ff50d-a8d4-4986-f503-eb246b958645"
      },
      "source": [
        "model = DepthNet(n_chans1=32).to(device=device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "training_loop(\n",
        "    n_epochs = 100, \n",
        "    optimizer = optimizer,\n",
        "    model = model,\n",
        "    loss_fn = loss_fn,\n",
        "    train_loader = train_loader,\n",
        ")\n",
        "\n",
        "all_acc_dict['depth'] = validate(model, train_loader, val_loader)"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-06-27 04:16:02.422374 Epoch 1, Training loss 43.37059458345175\n",
            "2020-06-27 04:16:06.314235 Epoch 10, Training loss 0.03415515832602978\n",
            "2020-06-27 04:16:10.679681 Epoch 20, Training loss 0.01353844627737999\n",
            "2020-06-27 04:16:15.066114 Epoch 30, Training loss 0.008161492645740509\n",
            "2020-06-27 04:16:19.471604 Epoch 40, Training loss 0.005757424980401993\n",
            "2020-06-27 04:16:23.868201 Epoch 50, Training loss 0.00441235676407814\n",
            "2020-06-27 04:16:28.241607 Epoch 60, Training loss 0.0035587064921855927\n",
            "2020-06-27 04:16:32.627812 Epoch 70, Training loss 0.0029716193675994873\n",
            "2020-06-27 04:16:36.995507 Epoch 80, Training loss 0.0025445520877838135\n",
            "2020-06-27 04:16:41.378202 Epoch 90, Training loss 0.002220548689365387\n",
            "2020-06-27 04:16:45.732214 Epoch 100, Training loss 0.0019669458270072937\n",
            "Accuracy train: 1.00\n",
            "Accuracy val: 1.00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFfthswJ_UXz",
        "colab_type": "text"
      },
      "source": [
        "Adding a skip connection a-la ResNet to this model amounts to adding the output of the first layer in the `forward` function to the input of the third layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3A4swFbj_hNo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NetRes(nn.Module):\n",
        "  def __init__(self, n_chans1=32):\n",
        "    super().__init__()\n",
        "    self.n_chans1 = n_chans1\n",
        "    self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
        "    self.conv2 = nn.Conv2d(n_chans1, n_chans1 // 2, kernel_size=3, padding=1)\n",
        "    self.conv3 = nn.Conv2d(n_chans1 // 2, n_chans1 // 2,kernel_size=3, padding=1)\n",
        "    self.fc1 = nn.Linear(4 * 4 * n_chans1 // 2, 32)\n",
        "    self.fc2 = nn.Linear(32, 2)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = F.max_pool2d(torch.relu(self.conv1(x)), 2)\n",
        "    out = F.max_pool2d(torch.relu(self.conv2(out)), 2)\n",
        "    out1 = out\n",
        "    out = F.max_pool2d(torch.relu(self.conv3(out)) + out1, 2)\n",
        "    out = out.view(-1, 4 * 4 * self.n_chans1 // 2)\n",
        "    out = torch.relu(self.fc1(out))\n",
        "    out = self.fc2(out)\n",
        "    return out    "
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghtk2zHiAh2c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "145fae87-38e2-4402-e75a-de36543c7039"
      },
      "source": [
        "model = NetRes(n_chans1=32).to(device=device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "training_loop(\n",
        "    n_epochs = 100,\n",
        "    optimizer = optimizer,\n",
        "    model = model,\n",
        "    loss_fn = loss_fn,\n",
        "    train_loader = train_loader,\n",
        ")\n",
        "all_acc_dict['res'] = validate(model, train_loader, val_loader)"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-06-27 04:22:56.585129 Epoch 1, Training loss 10.168318354524672\n",
            "2020-06-27 04:23:00.533128 Epoch 10, Training loss 0.02167535200715065\n",
            "2020-06-27 04:23:04.956596 Epoch 20, Training loss 0.009253103286027908\n",
            "2020-06-27 04:23:09.355721 Epoch 30, Training loss 0.00572286918759346\n",
            "2020-06-27 04:23:13.760081 Epoch 40, Training loss 0.004090409725904465\n",
            "2020-06-27 04:23:18.136835 Epoch 50, Training loss 0.003160446882247925\n",
            "2020-06-27 04:23:22.518225 Epoch 60, Training loss 0.002563226968050003\n",
            "2020-06-27 04:23:27.014480 Epoch 70, Training loss 0.0021487101912498474\n",
            "2020-06-27 04:23:31.540783 Epoch 80, Training loss 0.0018453225493431091\n",
            "2020-06-27 04:23:36.065701 Epoch 90, Training loss 0.0016141198575496674\n",
            "2020-06-27 04:23:40.434866 Epoch 100, Training loss 0.001432083547115326\n",
            "Accuracy train: 1.00\n",
            "Accuracy val: 1.00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMtu99BxA4r5",
        "colab_type": "text"
      },
      "source": [
        "In other words, we're using the output of the first activations as inputs to the last, in addition to the standard feed-forward path. This is also referred as **identity mapping**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQ9i-MgyBEiT",
        "colab_type": "text"
      },
      "source": [
        "Thinking back about back-propogation, we can appreciate that a skip connection, or a sequence of skip connections in a deep network, creates a direct path from the depper parameters to the loss. This makes their contribution to the gradient of the loss more direct, as partial derivatives of the loss with respect to those parameters have a change not to be multiplied by a long chain of other operations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrLsW4ULBjKE",
        "colab_type": "text"
      },
      "source": [
        "It has been observed that skip connections have a beneficial effect on convergence especially in the initial phases of training. ALso, the loss landscape of deep Residual Networks is a lot smoother than feed-forward networks of the same depth and width."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6hrk9r6BnxV",
        "colab_type": "text"
      },
      "source": [
        "Since the advent of ResNets, other architectures have taken skip connections to the next level. One in particular, DenseNet, proposed to connect each layer with several other layers downstream through skip connections, achieving state of the art results with fewer parameters. By now we know how to implement something like DenseNets: **just arithmetically add earlier intermediate outputs to downstream intermediate outputs.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nI-T4lUICAVa",
        "colab_type": "text"
      },
      "source": [
        "# Bulding very deep models in PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpPemj0dCJmK",
        "colab_type": "text"
      },
      "source": [
        "We talked about exceeding 100 layers in a convolutional neural network. The standard strategy is to define a building block, such as `(Conv2D, ReLU, Conv2D) + skip_connection` block, and then build the network dynamically in a `for` loop."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4d91CBajCiFh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# First create a module subclas\n",
        "# sole job: to provide computation for 1 block\n",
        "# that is one group of convolution, activation and skip connections\n",
        "class ResBlock(nn.Module):\n",
        "  def __init__(self, n_chans):\n",
        "    super(ResBlock, self).__init__()\n",
        "    self.conv = nn.Conv2d(n_chans, n_chans, kernel_size=3, padding=1, bias=False)\n",
        "    self.batch_norm = nn.BatchNorm2d(num_features=n_chans)\n",
        "    torch.nn.init.kaiming_normal_(self.conv.weight, nonlinearity='relu')\n",
        "    torch.nn.init.constant_(self.batch_norm.weight, 0.5)\n",
        "    torch.nn.init.zeros_(self.batch_norm.bias)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = self.conv(x)\n",
        "    out = self.batch_norm(out)\n",
        "    out = torch.relu(out)\n",
        "    return out + x"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2txwrycDQzu",
        "colab_type": "text"
      },
      "source": [
        "Since we're planning to generate a deep model, we are including **batch norm** in the block, since this will help avoid vanishing gradients during training. \n",
        "\n",
        "We'd now like to generate a 100-block network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rAq2_NLDf6t",
        "colab_type": "text"
      },
      "source": [
        "First in `init`, we create a `nn.Sequential` containing a list of `ResBlock` instances. **`nn.Sequential` will ensure that the output of one block goes as input to the next.** It will also ensure that all parameters in the block are visible to `Net`. Then, in `forward`, we call the sequential to traverse the 100 blocks and generate the output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wdnEm8IeD5E6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NetResDeep(nn.Module):\n",
        "  def __init__(self, n_chans1=32, n_blocks=10):\n",
        "    super().__init__()\n",
        "    self.n_chans1 = n_chans1\n",
        "    self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
        "    self.resblocks = nn.Sequential(* [ResBlock(n_chans=n_chans1)] * n_blocks)\n",
        "    self.fc1 = nn.Linear(8 * 8 * n_chans1, 32)\n",
        "    self.fc2 = nn.Linear(32, 2)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = F.max_pool2d(torch.relu(self.conv1(x)), 2)\n",
        "    out = self.resblocks(out)\n",
        "    out = F.max_pool2d(out, 2)\n",
        "    out = out.view(-1, 8 * 8 * self.n_chans1)\n",
        "    out = torch.relu(self.fc1(out))\n",
        "    out = self.fc2(out)\n",
        "    return out"
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6j8_RBCvGIjJ",
        "colab_type": "text"
      },
      "source": [
        "In the implementation we parameterized the actual number of layers, which is important for experimentation and reuse."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uN3cHBdoFR9l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "fc986a5b-735c-4e29-f5e3-856107083138"
      },
      "source": [
        "model = NetResDeep(n_chans1=32, n_blocks=100).to(device=device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=3e-3)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "training_loop(\n",
        "    n_epochs = 10,\n",
        "    optimizer = optimizer,\n",
        "    model = model,\n",
        "    loss_fn = loss_fn,\n",
        "    train_loader = train_loader,\n",
        ")\n",
        "all_acc_dict[\"res deep\"] = validate(model, train_loader, val_loader)\n"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-06-27 04:47:11.578882 Epoch 1, Training loss 3.7905080318450928\n",
            "2020-06-27 04:48:36.084015 Epoch 10, Training loss 0.0\n",
            "Accuracy train: 1.00\n",
            "Accuracy val: 1.00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ED4GGuMCFrPP",
        "colab_type": "text"
      },
      "source": [
        "All the above shouldn't encourage us to seek depth on a dataset of 32x32 images, but it clearly demonstrates how this can be achived on more challenging datasets, like ImageNet. The above also probides the key elements for understanding existing implementations for models like ResNet for instance in `torchvision`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "of6cNf95Gs4g",
        "colab_type": "text"
      },
      "source": [
        "## Take home"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8HIKFRrG7h9",
        "colab_type": "text"
      },
      "source": [
        "We should make every effot to proficiently translate the math behind a paper into actual PyTorch code, or at least to understand code that others have written with the same intention."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJ5doxWjHIJ1",
        "colab_type": "text"
      },
      "source": [
        "### Plots"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wft3rnJ4Hy5q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "outputId": "390aa761-0766-4c2c-89db-e3ca91c7d626"
      },
      "source": [
        "trn_acc = [v['train'] for k, v in all_acc_dict.items()]\n",
        "val_acc = [v['val'] for k, v in all_acc_dict.items()]\n",
        "\n",
        "width = 0.3\n",
        "plt.bar(np.arange(len(trn_acc)), trn_acc, width=width, label='train')\n",
        "plt.bar(np.arange(len(val_acc)) + width, val_acc, width=width, label='val')\n",
        "plt.xticks(np.arange(len(val_acc)) + width/2, list(all_acc_dict.keys()), rotation=60)\n",
        "plt.ylabel('accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.ylim(0.7, 1)\n",
        "plt.show()"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAErCAYAAADEyxRmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3debhVZdnH8e9PRkdAwREUHHJIzYEc0tQy59ks0TI0lcoxy1LfTHlJU3vTyjTHyNRySFNJMcMpzSlwyHlAnMAJUXFIRfR+/3ieo8vDAvYR1l4b+H2ua1/svdbaZ93sc/a61zMrIjAzM2tvvroDMDOz1uQEYWZmpZwgzMyslBOEmZmVcoIwM7NSThBmZlaqsgQhabiklyU9OJ39knSqpLGS7pe0TmHfYElP5MfgqmI0M7Ppq7IEcR6w9Qz2bwOslB9DgDMAJC0KHAusD6wHHCupV4VxmplZicoSRETcArw6g0N2As6P5E6gp6SlgK2AURHxakS8BoxixonGzMwqUGcbxDLAc4XX4/O26W03M7Mm6lx3ALNC0hBS9RQLLrjguqusssqn/lkPTJjcoePX0LiOnWDptTt2fDuOz/EVOb5PcnyfPr677777lYjoU7avzgQxAehXeN03b5sAbNZu+81lPyAizgbOBhg4cGCMGTPmUwfT/8hrOnT8mO57duwEQz99bOD4HN8nOb5PcnyfPj5Jz0xvX51VTCOAb+XeTBsAkyPiBeA6YEtJvXLj9JZ5m5mZNVFlJQhJF5FKAr0ljSf1TOoCEBFnAiOBbYGxwH+BffK+VyX9DBidf9SwiJhRY7eZmVWgsgQREXvMZH8AB05n33BgeBVxmZlZYzyS2szMSjlBmJlZKScIMzMr5QRhZmalnCDMzKyUE4SZmZVygjAzs1JOEGZmVsoJwszMSjlBmJlZKScIMzMr5QRhZmalnCDMzKyUE4SZmZVygjAzs1JOEGZmVsoJwszMSjlBmJlZKScIMzMr5QRhZmalnCDMzKyUE4SZmZVygjAzs1JOEGZmVsoJwszMSjlBmJlZKScIMzMrVWmCkLS1pMckjZV0ZMn+5STdIOl+STdL6lvY94Gk+/JjRJVxmpnZtDpX9YMldQJOB7YAxgOjJY2IiIcLh/0SOD8i/ijpy8AJwF553zsRsVZV8ZmZ2YxVWYJYDxgbEeMiYgpwMbBTu2NWA27Mz28q2W9mZjWpMkEsAzxXeD0+byv6D7Brfr4LsLCkxfLr7pLGSLpT0s4VxmlmZiXqbqQ+HNhU0r3ApsAE4IO8b7mIGAjsCfxa0grt3yxpSE4iYyZOnNi0oM3M5gVVJogJQL/C675520ci4vmI2DUi1gZ+kre9nv+dkP8dB9wMrN3+BBFxdkQMjIiBffr0qeQ/YWY2r6oyQYwGVpI0QFJXYBDwid5IknpLaovhKGB43t5LUre2Y4CNgGLjtpmZVayyBBERU4GDgOuAR4BLI+IhScMk7ZgP2wx4TNLjwBLA8Xn7qsAYSf8hNV6f2K73k5mZVayybq4AETESGNlu2zGF55cBl5W873ZgjSpjMzOzGau7kdrMzFqUE4SZmZVygjAzs1JOEGZmVsoJwszMSjlBmJlZKScIMzMr5QRhZmalnCDMzKyUE4SZmZVygjAzs1JOEGZmVsoJwszMSjlBmJlZKScIMzMr5QRhZmalnCDMzKyUE4SZmZVygjAzs1JOEGZmVsoJwszMSjlBmJlZKScIMzMr5QRhZmalnCDMzKyUE4SZmZVygjAzs1JOEGZmVqrSBCFpa0mPSRor6ciS/ctJukHS/ZJultS3sG+wpCfyY3CVcZqZ2bQqSxCSOgGnA9sAqwF7SFqt3WG/BM6PiDWBYcAJ+b2LAscC6wPrAcdK6lVVrGZmNq0qSxDrAWMjYlxETAEuBnZqd8xqwI35+U2F/VsBoyLi1Yh4DRgFbF1hrGZm1k6VCWIZ4LnC6/F5W9F/gF3z812AhSUt1uB7kTRE0hhJYyZOnDjbAjczs/obqQ8HNpV0L7ApMAH4oNE3R8TZETEwIgb26dOnqhjNzOZJnSv82ROAfoXXffO2j0TE8+QShKSFgK9GxOuSJgCbtXvvzRXGamZm7VRZghgNrCRpgKSuwCBgRPEASb0ltcVwFDA8P78O2FJSr9w4vWXeZmZmTVJZgoiIqcBBpAv7I8ClEfGQpGGSdsyHbQY8JulxYAng+PzeV4GfkZLMaGBY3mZmZk1SZRUTETESGNlu2zGF55cBl03nvcP5uERhZmZNVncjtZmZtSgnCDMzK+UEYWZmpZwgzMyslBOEmZmVcoIwM7NSThBmZlbKCcLMzEo5QZiZWSknCDMzK9VQgpD0V0nbFSbWMzOzuVyjF/zfAXsCT0g6UdLKFcZkZmYtoKEEERHXR8Q3gHWAp4HrJd0uaR9JXaoM0MzM6tFwlVFeCnRvYD/gXuA3pIQxqpLIzMysVg1N9y3pCmBl4AJgh4h4Ie+6RNKYqoIzM7P6NLoexKkRcVPZjogYOBvjMTOzFtFoFdNqknq2vchLgR5QUUxmZtYCGk0Q+0fE620vIuI1YP9qQjIzs1bQaILoJEltLyR1ArpWE5KZmbWCRtsg/k5qkD4rv/5O3mZmZnOpRhPEEaSk8L38ehRwbiURmZlZS2goQUTEh8AZ+WFmZvOARsdBrAScAKwGdG/bHhHLVxSXmZnVrNFG6j+QSg9TgS8B5wMXVhWUmZnVr9EEMX9E3AAoIp6JiKHAdtWFZWZmdWu0kfq9PNX3E5IOAiYAC1UXlpmZ1a3REsShwALAIcC6wDeBwVUFZWZm9ZtpgsiD4naPiLciYnxE7BMRX42IOxt479aSHpM0VtKRJfuXlXSTpHsl3S9p27y9v6R3JN2XH2d+qv+dmZl9ajOtYoqIDyRt3NEfnBPL6cAWwHhgtKQREfFw4bCjgUsj4gxJqwEjgf5535MRsVZHz2tmZrNHo20Q90oaAfwFeLttY0T8dQbvWQ8YGxHjACRdDOwEFBNEAIvk5z2A5xuMx8zMKtZogugOTAK+XNgWwIwSxDLAc4XX44H12x0zFPiHpIOBBYGvFPYNkHQv8AZwdETc2v4EkoYAQwCWXXbZhv4jZmbWmEZHUu9T0fn3AM6LiJMlbQhcIGl14AVg2YiYJGld4EpJn42IN9rFdTZwNsDAgQOjohjNzOZJjY6k/gOpxPAJEfHtGbxtAtCv8Lpv3la0L7B1/ll3SOoO9I6Il4H38va7JT0JfAbw6nVmZk3SaDfXq4Fr8uMGUrvBWzN5z2hgJUkDJHUFBgEj2h3zLLA5gKRVSVVZEyX1yY3cSFoeWAkY12CsZmY2GzRaxXR58bWki4B/zeQ9U/OguuuATsDwiHhI0jBgTESMAH4InCPpMFIJZe+ICEmbAMMkvQ98CHw3Il7t6H/OzMw+vUYbqdtbCVh8ZgdFxEhS19XitmMKzx8GNip53+XA5e23m5lZ8zTaBvEmn2yDeJG0RoSZmc2lGq1iWrjqQMzMrLU01EgtaRdJPQqve0raubqwzMysbo32Yjo2Iia3vYiI14FjqwnJzMxaQaMJouy4T9vAbWZmc4BGE8QYSadIWiE/TgHurjIwMzOrV6MJ4mBgCnAJcDHwLnBgVUGZmVn9Gu3F9DYwzXoOZmY292q0F9MoST0Lr3tJuq66sMzMrG6NVjH1zj2XAIiI12hgJLWZmc25Gk0QH0r6aMEFSf0pmd3VzMzmHo12Vf0J8C9J/wQEfJG8UI+Zmc2dGm2k/rukgaSkcC9wJfBOlYGZmVm9Gp2sbz/gUNKiP/cBGwB38MklSM3MbC7SaBvEocDngWci4kvA2sDrM36LmZnNyRpNEO9GxLsAkrpFxKPAytWFZWZmdWu0kXp8HgdxJTBK0mvAM9WFZWZmdWu0kXqX/HSopJuAHsDfK4vKzMxq1+EZWSPin1UEYmZmraXRNggzM5vHOEGYmVkpJwgzMyvlBGFmZqWcIMzMrJQThJmZlXKCMDOzUk4QZmZWqtIEIWlrSY9JGitpmjWtJS0r6SZJ90q6X9K2hX1H5fc9JmmrKuM0M7NpdXgkdaMkdQJOB7YAxgOjJY2IiIcLhx0NXBoRZ0haDRgJ9M/PBwGfBZYGrpf0mYj4oKp4zczsk6osQawHjI2IcRExBbgY2KndMQEskp/3AJ7Pz3cCLo6I9yLiKWBs/nlmZtYkVSaIZYDnCq/H521FQ4FvShpPKj0c3IH3mplZhepupN4DOC8i+gLbAhdIajgmSUMkjZE0ZuLEiZUFaWY2L6oyQUwA+hVe983bivYFLgWIiDuA7kDvBt9LRJwdEQMjYmCfPn1mY+hmZlZlghgNrCRpgKSupEbnEe2OeRbYHEDSqqQEMTEfN0hSN0kDgJWAf1cYq5mZtVNZL6aImCrpIOA6oBMwPCIekjQMGBMRI4AfAudIOozUYL13RATwkKRLgYeBqcCB7sFkZtZclSUIgIgYSWp8Lm47pvD8YWCj6bz3eOD4KuMzM7Ppq7uR2szMWpQThJmZlXKCMDOzUk4QZmZWygnCzMxKOUGYmVkpJwgzMyvlBGFmZqWcIMzMrJQThJmZlXKCMDOzUk4QZmZWygnCzMxKOUGYmVkpJwgzMyvlBGFmZqWcIMzMrJQThJmZlXKCMDOzUk4QZmZWygnCzMxKOUGYmVkpJwgzMyvlBGFmZqWcIMzMrJQThJmZlXKCMDOzUpUmCElbS3pM0lhJR5bs/5Wk+/LjcUmvF/Z9UNg3oso4zcxsWp2r+sGSOgGnA1sA44HRkkZExMNtx0TEYYXjDwbWLvyIdyJirariMzOzGassQQDrAWMjYhyApIuBnYCHp3P8HsCxFcZjZjaN97v2ZPw6R/Buj+UBlR/0yCOzdI5zdlyqQ8c/oks7doIG4uvevTt9+/alS5cuDf/YKhPEMsBzhdfjgfXLDpS0HDAAuLGwubukMcBU4MSIuLKqQM1s3jV+nSNYePmB9F+wM9J0EsTSq87SOd4f//rMDypYdb7pxDE9M4kvIpg0aRLjx49nwIABDf/YKhNERwwCLouIDwrblouICZKWB26U9EBEPFl8k6QhwBCAZZddtnnRmtlc490ey884OcwFJLHYYosxceLEDr2vykbqCUC/wuu+eVuZQcBFxQ0RMSH/Ow64mU+2T7Qdc3ZEDIyIgX369JkdMZvZPEdzdXJo82n+j1UmiNHASpIGSOpKSgLT9EaStArQC7ijsK2XpG75eW9gI6bfdmFmZhWorIopIqZKOgi4DugEDI+IhyQNA8ZERFuyGARcHBFRePuqwFmSPiQlsROLvZ/MzKrS/9TnS7aWbWvM0yduN8P9b0yezLVX/oXdB+/XoZ+77V4H8+fTfk7PHgt/6thmptI2iIgYCYxst+2Ydq+HlrzvdmCNKmMzM2sFb74xmUvO//00CWLq1Kl07jz9S/TIC35bdWgt00htZjZP+s0JQxn/zNN8fasv0rlzF7p260bfnl15dOzTPP6vK9n52z/guedf5N33pnDovnsw5JtfBaD/+tsx5toLeevtd9hm8z3ZeOONuf3221lmmWW46qqrmH/++Wc5Nk+1YWZWo0OPGkrf5fpz6XW3ctjRw3jkwfv5zbAf8fi/Us/+4Scfy91//zNjRl7IqcMvZtKr03aZfeKJJzjwwAN56KGH6NmzJ5dffvlsic0lCDOzFrL6WuswYNllPnp96vCLuOLamwB47vmXeOKpZ1ls0Z6feM+AAQNYa6008cS6667L008/PVticYIwM2sh8y+wwEfPb759DNff+m/u+Nt5LDD//Gy22/68+96Uad7TrVu3j5536tSJd955Z7bE4iomM7MaLbjQQvz37bdK901+8y169ViYBeafn0fHPsWd9zzQ1NhcgjAzK3j6kKWn3bj0NON0Z5uevRZlrYHrs+vmG9K9+/wsWhj0u/VmX+DMCy5j1U13ZeUV+rPBOs3t3OkEYWZWsxNPO7fdlqcA6NatK9deeFrpe56+6xoAei/aiwcffPCj7Ycffvhsi8tVTGZmVsoJwszMSjlBmJlZKScIMzMr5QRhZmalnCDMzKyUu7mamRWdvdns/XlDJ8/WH7fQShvx1hO3zdafOT0uQZiZWSmXIMzMavTrE4ay5FLLMGjv/QE445QTWabzm9x0+2hem/wm70+dynE/PoCdttqs6bG5BGFmVqOtdtiVf1x95Uev/3H1lQz+2vZc8fuTuee6P3PTX87ih8NO4ZOLbjaHSxBmZjVadfU1eXXSK7z84gu89uorLNKjB0suvhiHDT2ZW+66h/k0HxNenMhLEyex5OK9mxqbE4SZWc222G4nRo0cwaSXX2LLHXblT3+9lomTXuPua/9Ely5d6L/+dqXTfFfNVUxmZjXbaodduG7E5YwaOYItt9uJyW++xeK9F6VLly7cdNtonhn/Qi1xuQRhZlY05OZpt1U43TfAiiuvyttvvcXiSy5FnyWW5Bu7bsMOg7/PGpt/nYFrrsoqK/av9PzT4wRhZtYCLr/+9o+e9160F3f87Y+lxzVrDAS4isnMzKbDCcLMzEo5QZjZPC5qGWPQbJ/m/+gEYWbztO6TxzHp7alzdZKICCZNmkT37t079D43UpvZPK3vPScxniOY2GN5QOUHTX5kls7x0mvvdOj4RzSxYydoIL7u3bvTt2/fDv1YJwgzm6d1mfI6A+48asYHzeKMrNsceU2Hjn+6+54dO8FsnjG2TaVVTJK2lvSYpLGSjizZ/ytJ9+XH45JeL+wbLOmJ/BhcZZxmZjatykoQkjoBpwNbAOOB0ZJGRMTDbcdExGGF4w8G1s7PFwWOBQYCAdyd3/taVfGamdknVVmCWA8YGxHjImIKcDGw0wyO3wO4KD/fChgVEa/mpDAK2LrCWM3MrB1V1XIvaTdg64jYL7/eC1g/Ig4qOXY54E6gb0R8IOlwoHtEHJf3/xR4JyJ+2e59Q4Ah+eXKwGOV/GfK9QZeaeL5OsrxzRrHN2sc36xpZnzLRUSfsh2t0kg9CLgsIj7oyJsi4mzg7GpCmjFJYyJiYB3nboTjmzWOb9Y4vlnTKvFVWcU0AehXeN03bysziI+rlzr6XjMzq0CVCWI0sJKkAZK6kpLAiPYHSVoF6AXcUdh8HbClpF6SegFb5m1mZtYklVUxRcRUSQeRLuydgOER8ZCkYcCYiGhLFoOAi6PQGBIRr0r6GSnJAAyLiFerivVTqqVqqwMc36xxfLPG8c2aloivskZqMzObs3kuJjMzK+UEYWZmpZwgzMyslBNERSRNZ1pIm5PlXnVmTVfHNaVVBsrNdaIFW/8lzRcRH9Ydx5xG0goR8aSkDYCNgV/O7D1WTpJa4bshaQGga0S8PtODW0DbdzfHvT/QlXSD/6eIGF/VeV2CmE3y5IRI2krSTyQdKelLNcbTOf+7iKR+khZvSw6tVLqRNF/+d21J/euNZlqSugCrS7oKuAQYk7d3qjWwBhQ+280l7SlpDUmLN/H8bd+JFSV9BVrqxukXpPni5hRtn9svgK+Qbu4XAU6VtHFVJ3WCmA3yXdEHknoApwCPAIcCi+b9Czc7pjwOpRPwD2AYcL6kwyR1apUvaf7c2ko0Q4ALJX1XUseWvapQRLxPmizyTdJ4nsGStmybFkbS9vn33lLaPltJfYHfArsD/wvsLWkDSYtUfP6F83diGeBy4CRJD0j6Vh44WxtJ2wGfi4h/SOopaYikA1o16efSQ0haHugZETtExAnAr4B7gM0rO3eLXCvmCpIOJS1JNRy4NiI2ykXC7wFnRsTbTYpj74g4T9I3SHcbPyJNpf4t0qj1P0fEn5sRy4wUis3/A7Qt5yWgB3BuRFzbCvHl54sASwGbANsD44Anga0iYof6opwxSXsDC0fEbyVtAuwGLAD8B/h9RPy3gnN2Bn5MKm1tAzwREb+TtBPpb/E14DcRcf3sPneD8f0J+Fd+DAGWAF4HTomIR+uIqRF5kPHRwF4R8ae8bUXgUtLEqC/P7nO6BDF7PUGaN+oG4MS87dvAV5qRHJT0AQ6X9G/gy8AVEfEKcDNwOHAV8IWqY2lETg6LA98lfVH3J5V2XgZ+Kum4ZlaJlMUHIOkM0nT1kyLiHOAnwNuki+1p+ZiW+S4Vqnb6Ae8AG+Vkd0tEHALcSqrtme3JIetHSvJbkC6+3UgnvCoiNgbuJiWOpipUrQ4HNgL+RprpYQ/SjVPtk+PNxC+Ao4Chks7PVUu7ALdGxMtV/A26BDEb5aLzKcBqwEnAu6RFk74VEfc0OZbtgaGkusu9I+KhvL0L0Cki3m1mPNOjtDjUecDhEfF43rY68H1SPeufImJUfRGCpG7AmaQv42kRcXTe3hINrtMj6X7SXfwawAfAbwt3nsrVFpX8H/J34Zuk78JnSPOw3VH4O2w7f9M6TuQL6PxAH1L9/ZSIeDS3jxwfEes3I45GFX83kj5L+h1OBrqQFlTbi5RsvxIRb1fxWbbMXc+cqHCntnjO5gsAR5KKrl8j3RGf3Kzk0FavLGkzgDxd8HXA1ZJOlrRYRLzfKskB0rxbwC3AeZIOyXd5O5O+CDcAe9TZqC6pV0S8FxH7AJ8HtpX0kqTd2y6wdcVWptAwvQnwQER8OyI+T6qv3l/SXbldQjD7G40lrZwT6hak0svxwBXAqsA3JO1XbP9ocq+6I4BTSaX7VXJy6E2qwz+piXE0SgCSfkQqOfwB+HFEPBsR+5J61D1GWnHzy5V8lhHhxyw+SJMKXkG6qP24phi6kqppDgXuIpUa2vb1Ay4j1ZkvVPfnVYhrXVIbyeeBL5F6CT2WY10AuAA4oMb4NiQl/HXbPjdgQVLiOqHuz28GcXcmlWRHARsA3fL2HsAhwHwVnXd+YAfShWwCsHFh36rA/wDH1PSZrENqd+kDPNcWG6lKuE/dv7MZxN0beCg/vw7YLz9fO/+eO5NKaitWcX6XID6lQulhL9Kd2i6kL+POksZJ2j/vb9Zn3ImUpHYFVgImS1pCUveIeI5Uz7pxRLzVpHhKFT63/UgNbruRist3RsTupLrhrwOrky7Kv6srVlKSWoWUeLeXtBJp5cJngJ9Cy7U9tMWyB/Ae6UK4O7CNpL4RMTkiTo3U9jPb446Id0hVHsuQqle3l7RL3vcI6QL9URXX7D7/TAwEfk6qbvt3RPxL0gBSCadlStQlVgFGSNoC6BIR5+bf3enAZyNiakRcGBFjqzi52yBmgVL31ROAN4BjI3WJRNIg4LsRsVmT4liNVB95MemOqCfpwvAGqRF1uRzfas2IZ2ZyFcS9wPrAGaQEe5KkTYGnIuJZScuSakCea3JsxZ5LnSJ11dyG9HkG6UJzVkSc1sz685kp1On3I7XbbJKrcoaQku3LwCURcXeV58/PlyTd+W4EfA6YCCwMLB0Rg6o4/0xi2xXYF5hE6oW2Q0Q8IOlkYP6IOKDZMTUqt+X8L7AncGREXCTpAODLEbFb1ef3SOpPQdIWkRpOFyddjFcCdpN0L6lL38Wki3WzLEpqTxoEPA+cS6qu+Q7pj+stUiJrFYsBfwFWAPpFxDfz9p+SejE9GxHPNjuoQrfbvqTPa0FJS5G6bO4LfBZ4OyKehKbXn89Q28UZ2Bp4QVKXiHgD+KWkNYH9SF05Kz1/vtNdj1TNeSswNr9ek3QHX8eI/s1JnQwWIyWubSV9i1S9uUkT4+gQpbEkb0oaRbrxW1fSvqS2iQPzMZ2ig0s1dygGlyA6RtJ6wNKkbqPLR8Q9uRi9K+nifBep29nEJsXTdue4FvAz0hfxH6R68ktI1STdI2JSM+KZnnYX3zVIYwkGA8dFxImSBgN7RsRWdcYJIOlC4GlS6WtTUqPmmRHRig2ZH5G0GKkxehXgQlKd9dgqLyD5vG2/211ICf5qUvvNg6SxBeMKxza151fuzTcC+FFEnKw0FqN/3n1LRNzbrFgaUSi17khaSfMzwB9J1XNLAB8CT0fEU81ItE4QHSCpc0RMzc/3JNWf30ZqTH2XdJe2IfCdaNIKeIUEcTVwJXAfqepmY9Id4zXAyFa525U0HLguIi6RdAypCqIvqfg/LCLurPquaCbxLU5KrN+IiOfzttWBg4FDIuK9OuJqVK7b35RU4nkZ+CdwT1Q4X0/h3GcCl0XE9UrdqX8GbEuqDnml6vNPJ6auwD7AD0hdfo9pKwG2qtzG8CCpNPgX4K+5CrZnNHnuqJZpYJtD/FjSryUtQRq9+DtSNdOJpDvi00ltD01bHjUnh6VIPUguiogxEXE6KVlsAixSd3Joa5CUtCEwFXg87zqXdOE9AtgjJwfVlRwAIo1GHQ0cVNg8kTS4sNLpKT6NQqP/1pJ+QCo5fADsTRq4+V1SG1TVcWxDqoLbVtJSkbpTH0nqzdS/6vNPT0RMiYizSB1IHgf+KulcSZ1arYtywY7ASGAKqd2rbXLIMySt2sxAnCAalLP6baQL3LmkrmU3kOpVR5FGLf8caFbJYYl8l0ZEvECa/+msXNUEKYG9TCpB1KpQpbABqbH3h5Lmj4jn8+PmiJjc7timKfboyRfck4H1JD0i6XDg96QR6RNbrNdS2xxgi5BG2b4H3EHqKTSctK7xkRFxWxPCeRa4HlgR2FHS13LjcN+IGNOE889QRLwWEW2NvQ9ExAd1/K1Nj6SlCy9Hk8aQXAOcnX/H25Ma+R9palwt9Bm1vHzH0ZtUfbMraezBWRFxo9KcKN0j4sEmxXI86S5jXES8kOufDyOVJAaQ2h7ujIihzYinUblr4a9IF5JfRsR5NcfTVkXXndRI3pdUNXcUqaruC6Rqu5H5uJYbPS3ph8AaEbF3YdsfSXMt3VLheds+u4VJ81SNI40Z2ZdUdTiO1HPqyqpimBvkm44bSSXVH0XE07mn0r6kiRb7karqjo6IG5pZBesE0YB2XR+7RsSU3J1wG1JD0hvA0Gb3vJE0P/AwqQHrJ6TSy8qki+8rrfTFzMlVhc9xW/JslBGxR51x5YvcL0m9qv6PdJf5dVJb0hXtj60p1OmSNBD4XqTRtW3bjiP1mz+ionN2jjRj8DqksQQvAF8k/f11IX03dgVeIl38/t6Kn12ryDd4x5B6Vv0uPzYj9cB6F3iwju+zu7k2oHBR+x9gRaU++j8AzgfuJI1B6EkqZkmJMFcAAAukSURBVFeukLA2JlVzBWnisRHAzyPi5mbE0RH54hBtdz8RMRIYmUtetSgkhyVJvUN+EBFPAbdLuhLYXdKItru1Fr7APQYsIukR0o3C66Q2sb2hmm6lbZ01SA3RvyX1sFk492YaQOpJN5o0sr9LC392LSEiJkl6ldQ4vVd+/DTyvF9tmn6TEi0wnLyVH+RpCUhtDLeQEsHrwOp1xpOfL0oahfz1/PyUHNuguj+3Bv4fndu97lljLCeR6u+PK2zrRqoiWaHuz6ok3k7531VJd+1fyq93Jt2tnwQclLepgvN/jtRgvyCpK/D8wL+BNfP+U4F96v6c5oQHH9fiHEhqw5mPNCXKIOBRUnf6flQ0PcrMHq5iapCk35Hu0JcCNouIwUorxn2DNF/QlCbHcwipEas7aRDQ06SpxdcEXo10J1yLXJ20UES8WXhNtPtjK/T5/gGpT3rTGjPb34nlBtVhpMb+m0lf0qUi4uAWrlp6GHiAVMXZCTgnIu5od8xsjV1pAaBjST2kbiSNZdkUuCoijlGadfQyYIOImNyqn12rUZp6ZrHIY21yu8RxpLmjhtT1GbZMj4xWVei1ciWpF87BwA/ztj2BF2pIDivmOP4GrEXqrfIV4CLgyTqTQ/Zj4AJJ/5T0xciKB+Rqjw9yW85epJWxmqYtHknrS9qcdMFbI8dxJGnywOPawm1mbDNS6Na6OfCXSPNXnU7qvbSfpPPyRRyopFrsNdJ3YXHSuJ8JpNLX7pL+F/g1aUr0yWqh1QvnAPcBB0k6Fj6q1v4ccGFEhGpa7c4liOko1E93IlU3LEnq/rg0qTHpi6QpjTeMGsYZ5Ebeo0n1z8NJvRy2zfFUtRBMI3F9gVSi2YJUujoE2D0i7prO8eeReoLdUba/ohjbSi4HAl8FxpN6pE0iJf8lSd1Gv0Bq/P1bs2JrhKRepOqHSyItAISknqTS48qRFjWq4rxtn9sipFJCN9KEgPeTStbvAH+IiCeqOP/cTmlOtZNINyo3k7oIf6XWmJwgyunj6QOOA/4bET9XWkrxu6QBQY+TFkC5s8lxbUqaMO560tQe+5CqZ+6UtGA0aVnTGcT3N+DqSIOTkPR90mC9Yfn1MsCL+UKzGamuvPJJx0riXIhUPbMBqYqmNyk5PBFpoCGSdiZNVdGUrsuNKNy4/JB0g3ALqQripby/W0S8V2XVjqRzgcci4v9yD6qvkboE30GaBv2NKs47tyrp4bcBqS1xQqS5mGqbWcBVTCXyl+vDXJ+6C3BmTg6HkmYb/V5E/KqG5CDSrJhTSXfpfyJN7XGRpIF1JgdlwJ+Ba9vaHUiDCNfPx+xAuoC0/bG3VdnVoRtpMrkpEfFOpFljLydNjb1k/hu4spWSA3xcZRQRJ5N6Dr0CPCTptHzIlOJxs5vS4Mw3+HgZ0TGRutK+Djzv5DBz+nhRp4XyjWjk6818+e/uzoh4tK0Nr67kAE4QpQpfro1IE2VBmt3zi8AvJH2jrrgi4uqI+E1EbEjqQXIbqTSzaB0xFQhYPCIuItVLk5PqY0CPXPV0KGluGQAi4sRIo8CbE+DH9ferkebRmkLqavvVfMgA4P2IeLHV687zOIQp8fHKYgMlLV913JGmtL8QWEfS3pLWyL/n5UidOOpY62GOUqiSHkqaWPGj7a32d+cqphKFYvz6wF9Jd2m/iYjhSlMvdIuI4+uN8mNKUzu/X3MMq5IaeH8VEf/Tbt8hpIbfa/MFrfJpimdE0h9IU2eMyMl+MKnd4SnSuhn3qYXWepiefCf6ibmrmhF3TgBbkBryNyYl2lsjYuic8LnVqVB1/SNgYETsrrQu+9akWVpvrznET3CCmAlJC5Cm9X5Q0udJ8/JsGREv1hxay8nVD0cB3yPNAfTHvH010mp320fEE3VcRCStlS/8a5EmB7wqIi4ufGHXJLU3/LdVu2ZOL65iss2f9SPNiF/SgqQpXRYEnsk3VS352bUSpRkQbiS1ZwZwOOlzfB04ONLKfC3BCaKg0EtjEGmRk3dIo0HvjojnJB1B+sxOrDXQFqc0bcAfST1bDoiIuyQNiCbNYV8ST7Hv/lKk3kljSQMLH40ae33NTCMX3MLf7fbAVyNinyaFZx1UuCE5jFQKW5TUnfpfwFWkThsP1BljkRNEVqhW6kX6ZX2f1L/8PlI3yFHAP+psMJrT5LvyvwN/i4jv1BjHAqTBXJuS7theJrUvLUSaEuLvpETRklUjknqTRta+Suq19HLksTfFBCLpetI6Fi/VFqyVKiSGtjmsPkf6+3srIv6jNFBuh4jYqeZQP8FzMWWFu7SDgbNI9dGTSHOxn05a2elp0khba0BE3J/v3leAeia7y3fX/5V0G6kbaxdS3/3bSF/QnYDlIuLQZsY1M7l9oVeklQB/R5orqm2CyGsk3UqaIA/SHFcHADc6ObSmws3HkTnhTyENgrtfaZXF7Uml3Frb59pzCYJp7sI2Be4FDiCtjfxnSUcCXSP35bc5z3T67m9AKkFcEREPt1IDq9IYjK+SxhZsGRE75+2DSUntbeAXEfGA0piOe4HPRpNH9dvMFUoPe5F6z51Dugn9PPAiqbtyt0jTfLdUG467uSZt/ZL3IN3tHkAaMX2o0ojlb5OmF7A50Az67k8GJkfEw3l7SySH7DnS+uarAAMkbQeQG/73JbWntJUgPgR2c3JoTYW/q02AHwHLAJdGWtJ2U9KMA8/kY1smOYBLECiv86o0WdtQ0oLrbwOrkZLFeODmiDht+j/FWp3SugVHk/rq302qKhwN7NKKd27wUS+hNUgliT6k6TUuj8JUFq1U6rFpFUoP3Umz7e5ImmNpjbz9KuCmiPh1rYFOxzydICT1B24i1fEuT1re795cR7gVsBJwfN1jDGzWzSl99wudJRYgtZmcROrpsgFpipV+wKiIuLDGMK0Bhd/l0qRBrUeTJrJ8n7SOzOLAzhGxfo1hztA8nSAAJH2ZNJ/RVsCZEXFMYd99pH7Jt9YVn81erd53v3DH+QugT7HLqqRVSAluTESMri1Ia0ghQRxGasM8SWmepS1I68q8AoyIiIdaqWG6aJ5PEJAmOCNNOX0EaQnPU0nz6/84ap5N0eY9kpYAriUtBDRZ0gK5J1Y/0nxHLXchsXK5F9+9pPXhdyxsb8mE0J4bqYGIeC8iziUNjnuAtBDPOaSEYdY0+a7zJVIj9LoAOTl0Jo3XWKHO+KxjImICsAewqKS7cltnrRPwdYRLECWUZnFdLyL+UHcsNm+Q1DUipkhandQxYi9S75ZrSDPkfhfoERFDWqlKzBqjNFHkHsAQ0mDN3YGXWv336ARhVjNJC5MazseRphw/NCJuUJoBdz/SqoGjgP+LiFdaqVHdOkZpYr59gV/PCZ1fnCDMapYbzg8gTYc+kTSt/H/j4wVkuham1nBymEvMCb9LJwizFiHpZNKULu+Sppm/mDQFw6YRcXidsdm8yQnCrEbtpnnpHhHvStqNVFf9NKmh+tcRcYHbHqzZnCDMalKYpnsV4CDSdCBPAiNJE0VuTprt0+NwrBZOEGY1k3QDcD5pIrcepGlArieN9H4xH+PSgzWdx0GY1UjS5qT1Hf5ImsTtOKAraYnWDduOc3KwOrgEYVYjSSuSpv1YANgnj3NYi7R067ci4r1aA7R5mhcMMmsySV0i4n1JW5FmaV0WELCxpG+SVo+7KSLemxO6QtrcyyUIsyaS1CciJhYW+bkaeIk0m/DKpHXQ/5PXqzCrlROEWZPkKcdvJM2z9CzwRkScKqkn8FnSKncnR8Rz+XiXHqxWbqQ2a5Lc0Lw/qTvr3sDWefvrEXEbsBSwZ+F4JwerlROEWRNFxNg8KvrbpBk+R0sanMdC9AGugI9KG2a1chWTWU3yDJ/fJHVt7UNanOocVy1Zq3CCMKtZnuFzd+CciJjqQXHWKpwgzFqISw/WSpwgzMyslBupzcyslBOEmZmVcoIwM7NSThBmZlbKCcLMzEo5QZiZWan/BygUl80bvAIbAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UpeSHI7oIK3K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}