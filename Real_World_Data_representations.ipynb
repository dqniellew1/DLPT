{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Real-World_Data_representations.ipynb",
      "provenance": [],
      "mount_file_id": "14LZm0HE1LCjwhr_oXQiPcP_tWpe9fv10",
      "authorship_tag": "ABX9TyPbqNOsZ8b2HBpy4D8LZZRz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dqniellew1/DLPT/blob/master/Real_World_Data_representations.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4PyiJV8P9VG",
        "colab_type": "text"
      },
      "source": [
        "# Images\n",
        "\n",
        "`imageio` package to handle images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XV55UkiQYA9S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "drive_dir = 'drive/My Drive/dlwpt-code/data/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TwIJVKm7QFdD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import imageio"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hN3S_cwHQKEP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "69c0d2d2-bebd-4322-c034-15afbe52ccf4"
      },
      "source": [
        "# Loading an image\n",
        "img_arr = imageio.imread(drive_dir + '/p1ch4/image-dog/bobby.jpg')\n",
        "img_arr.shape"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(720, 1280, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTPewbZIS0Sm",
        "colab_type": "text"
      },
      "source": [
        "PyTorch require for input tensors to be in the format of `C x H x W`.\n",
        "We can use `permute` method to get the new format."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33nMKeMbSd3Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "img = torch.from_numpy(img_arr)\n",
        "out = img.permute(2, 0, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68_1_CN-TflE",
        "colab_type": "text"
      },
      "source": [
        "To store images in a batch in tensor, the dimensions are a `N x C x H x W` \n",
        "tensor.\n",
        "\n",
        "A more efficient alternative to using `stack` to build up the tensor, we can pre-allocate a tensor of approiprate size and fill it with images loaded from a directory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GzlyBSx3TJLL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 100\n",
        "batch = torch.zeros(100, 3, 256, 256, dtype=torch.uint8)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zy6LWZx4TMQ-",
        "colab_type": "text"
      },
      "source": [
        "## Loading images from a directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dlvm503eUbdf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "\n",
        "data_dir = 'drive/My Drive/dlwpt-code/data/p1ch4/image-cats/'\n",
        "filenames = [name for name in os.listdir(data_dir) if os.path.splitext(name)[-1] == '.png']\n",
        "for i, filename in enumerate(filenames):\n",
        "  img_arr = imageio.imread(os.path.join(data_dir, filename))\n",
        "  img_t = torch.from_numpy(img_arr)\n",
        "  img_t = img_t.permute(2, 0, 1)\n",
        "  img_t = img_t[:3] # Keep only the first three channels. Only RGB inputs.\n",
        "  batch[i] = img_t"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uz9vFJt3WAGS",
        "colab_type": "text"
      },
      "source": [
        "Neural networks work best when the input data ranges from 0 to 1, or from -1 to 1. Typically we want to cast a tensor to floating point and normalize the values of the pixels.\n",
        "\n",
        "Normalizations is trickier as we have to decide the range of input between (0 to 1) or (-1 to 1). One possibility is to just divide the values of pixels by 255. (The maximum nimber representable number in 8-bit unsigned)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9f6eddVRVO5n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch = batch.float()\n",
        "batch /= 255.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3F05bHIwWyLT",
        "colab_type": "text"
      },
      "source": [
        "Another way is to compute the **mean** and **std** of the input data and scale it so that the output has **zero mean** and **unit std** across each channel."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CiDbA4k4Wnn_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_channels = batch.shape[1]\n",
        "for c in range(n_channels):\n",
        "  mean = torch.mean(batch[:, c])\n",
        "  std = torch.std(batch[:, c])\n",
        "  batch[:, c] = (batch[:, c] - mean) / std"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAXYecOQWoP6",
        "colab_type": "text"
      },
      "source": [
        "Above is an example of normalizing a single image, because we do not know yet how to operate on an entire dataset. It is good practice to compute the mean and standard deviation on the entire training data in advance and then subtract and divide by these fixed pre-computed quantities."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qOtl1i2XUJz",
        "colab_type": "text"
      },
      "source": [
        "# Volumetric Data\n",
        "\n",
        "Consists of an added dimension after channel which is **depth**, leading to a 5D tensor of shape:\n",
        "\n",
        " `N x C x D x H x W`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPPeKKSHYxt7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82
        },
        "outputId": "4eb21adb-8afe-4bae-d24a-5647f6bef896"
      },
      "source": [
        "# Loading in a sample CT scan\n",
        "dir_path = drive_dir + \"p1ch4/volumetric-dicom/2-LUNG 3.0  B70f-04083\"\n",
        "vol_arr = imageio.volread(dir_path, 'DICOM')\n",
        "vol_arr.shape"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading DICOM (examining files): 1/99 files (1.0%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2/99 files (2.0%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b3/99 files (3.0%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b4/99 files (4.0%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b5/99 files (5.1%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b6/99 files (6.1%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b7/99 files (7.1%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b8/99 files (8.1%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b9/99 files (9.1%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/99 files (10.1%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/99 files (11.1%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/99 files (12.1%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/99 files (13.1%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b14/99 files (14.1%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b15/99 files (15.2%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/99 files (16.2%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b17/99 files (17.2%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b18/99 files (18.2%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/99 files (19.2%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/99 files (20.2%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b21/99 files (21.2%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b22/99 files (22.2%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b23/99 files (23.2%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b24/99 files (24.2%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b25/99 files (25.3%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b26/99 files (26.3%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b27/99 files (27.3%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b28/99 files (28.3%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b29/99 files (29.3%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b30/99 files (30.3%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b31/99 files (31.3%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32/99 files (32.3%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b33/99 files (33.3%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b34/99 files (34.3%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b35/99 files (35.4%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b36/99 files (36.4%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b37/99 files (37.4%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b38/99 files (38.4%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b39/99 files (39.4%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b40/99 files (40.4%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b41/99 files (41.4%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b42/99 files (42.4%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b43/99 files (43.4%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b44/99 files (44.4%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b45/99 files (45.5%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b46/99 files (46.5%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b47/99 files (47.5%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b48/99 files (48.5%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b49/99 files (49.5%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b50/99 files (50.5%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b51/99 files (51.5%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b52/99 files (52.5%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b53/99 files (53.5%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b54/99 files (54.5%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b55/99 files (55.6%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b56/99 files (56.6%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b57/99 files (57.6%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b58/99 files (58.6%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b59/99 files (59.6%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b60/99 files (60.6%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b61/99 files (61.6%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b62/99 files (62.6%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b63/99 files (63.6%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b64/99 files (64.6%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b65/99 files (65.7%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b66/99 files (66.7%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b67/99 files (67.7%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b68/99 files (68.7%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b69/99 files (69.7%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b70/99 files (70.7%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b71/99 files (71.7%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b72/99 files (72.7%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b73/99 files (73.7%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b74/99 files (74.7%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b75/99 files (75.8%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b76/99 files (76.8%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b77/99 files (77.8%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b78/99 files (78.8%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b79/99 files (79.8%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b80/99 files (80.8%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b81/99 files (81.8%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b82/99 files (82.8%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b83/99 files (83.8%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b84/99 files (84.8%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b85/99 files (85.9%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b86/99 files (86.9%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b87/99 files (87.9%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b88/99 files (88.9%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b89/99 files (89.9%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b90/99 files (90.9%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b91/99 files (91.9%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b92/99 files (92.9%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b93/99 files (93.9%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b94/99 files (94.9%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b95/99 files (96.0%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b96/99 files (97.0%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b97/99 files (98.0%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b98/99 files (99.0%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b99/99 files (100.0%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b99/99 files (100.0%)\n",
            "  Found 1 correct series.\n",
            "Reading DICOM (loading data): 18/99  (18.2%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b49/99  (49.5%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b81/99  (81.8%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b99/99  (100.0%)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(99, 512, 512)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCQy0VeKbL3B",
        "colab_type": "text"
      },
      "source": [
        "In this case, the layout is different from what PyTorch expects, due to having no channel information. We will have to make room for the `channel` dimension using `unsqueeze`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z0nTJnFscDbV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "bac3dd64-b7d0-41fb-9e10-c692a03d5b2a"
      },
      "source": [
        "vol = torch.from_numpy(vol_arr).float()\n",
        "vol = torch.transpose(vol, 0, 2)\n",
        "vol = torch.unsqueeze(vol, 0)\n",
        "\n",
        "vol.shape"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 512, 512, 99])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cK6-uM2ncMt0",
        "colab_type": "text"
      },
      "source": [
        "# Tabular Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vi1VaWWucvVQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import csv\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5-XDZCPd__8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        },
        "outputId": "f4b6fe24-b4ab-49b6-d84f-69b546564d4b"
      },
      "source": [
        "wine_path = drive_dir + 'p1ch4/tabular-wine/winequality-white.csv'\n",
        "wineq_numpy = np.loadtxt(wine_path, dtype=np.float32, delimiter=';', skiprows=1)\n",
        "wineq_numpy"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 7.  ,  0.27,  0.36, ...,  0.45,  8.8 ,  6.  ],\n",
              "       [ 6.3 ,  0.3 ,  0.34, ...,  0.49,  9.5 ,  6.  ],\n",
              "       [ 8.1 ,  0.28,  0.4 , ...,  0.44, 10.1 ,  6.  ],\n",
              "       ...,\n",
              "       [ 6.5 ,  0.24,  0.19, ...,  0.46,  9.4 ,  6.  ],\n",
              "       [ 5.5 ,  0.29,  0.3 , ...,  0.38, 12.8 ,  7.  ],\n",
              "       [ 6.  ,  0.21,  0.38, ...,  0.32, 11.8 ,  6.  ]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3cPq9vceWaV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "outputId": "9ce05020-543b-47a0-de9a-1586379b9670"
      },
      "source": [
        "col_list = next(csv.reader(open(wine_path), delimiter=';'))\n",
        "wineq_numpy.shape, col_list"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((4898, 12),\n",
              " ['fixed acidity',\n",
              "  'volatile acidity',\n",
              "  'citric acid',\n",
              "  'residual sugar',\n",
              "  'chlorides',\n",
              "  'free sulfur dioxide',\n",
              "  'total sulfur dioxide',\n",
              "  'density',\n",
              "  'pH',\n",
              "  'sulphates',\n",
              "  'alcohol',\n",
              "  'quality'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRK_2_mYelAa",
        "colab_type": "text"
      },
      "source": [
        "Convert the NumPy array into a PyTorch tensor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ua8W6OB2e7wk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "522d98c6-391a-41d7-b4be-d4fc5c3d14c8"
      },
      "source": [
        "wineq = torch.from_numpy(wineq_numpy)\n",
        "\n",
        "wineq.shape, wineq.dtype"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([4898, 12]), torch.float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crSMSMchfCHJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        },
        "outputId": "efba4a74-6072-48aa-8d9f-f26185bf2238"
      },
      "source": [
        "data = wineq[:, :-1]\n",
        "data, data.shape"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[ 7.0000,  0.2700,  0.3600,  ...,  3.0000,  0.4500,  8.8000],\n",
              "         [ 6.3000,  0.3000,  0.3400,  ...,  3.3000,  0.4900,  9.5000],\n",
              "         [ 8.1000,  0.2800,  0.4000,  ...,  3.2600,  0.4400, 10.1000],\n",
              "         ...,\n",
              "         [ 6.5000,  0.2400,  0.1900,  ...,  2.9900,  0.4600,  9.4000],\n",
              "         [ 5.5000,  0.2900,  0.3000,  ...,  3.3400,  0.3800, 12.8000],\n",
              "         [ 6.0000,  0.2100,  0.3800,  ...,  3.2600,  0.3200, 11.8000]]),\n",
              " torch.Size([4898, 11]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ARVsBRX1hMiM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "f90546a0-5f87-46cf-e376-5823ed26fbd9"
      },
      "source": [
        "target = wineq[:,-1]\n",
        "target, target.shape"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([6., 6., 6.,  ..., 6., 7., 6.]), torch.Size([4898]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TjrpyCmLhPLd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "e04e4e62-5c82-4a1c-d67a-9e08b52430dc"
      },
      "source": [
        "# Treating labels as an integer vector of scores:\n",
        "target = wineq[:, -1].long()\n",
        "target"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([6, 6, 6,  ..., 6, 7, 6])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTqU8lXghu-j",
        "colab_type": "text"
      },
      "source": [
        "In PyTorch we can achieve `one-hot` encoding using the `scatter_` method, which fills the tensor with values from a source tensor along the indices provided as arguments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1yv52xUFk4Vy",
        "colab_type": "text"
      },
      "source": [
        "The `scatter_` method reads plainly as for each row, take the index of the target label (which conincides with the score in our case) and use it as the column index to set the value to 1.0. The end-result is a tensor encoding categorical information."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-TzLR1qh6nX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        },
        "outputId": "60d42d4f-6a36-4474-8972-7d7dd0deef9c"
      },
      "source": [
        "target_onehot = torch.zeros(target.shape[0], 10)\n",
        "\n",
        "target_onehot.scatter_(1, target.unsqueeze(1), 1.0)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSbs5dTqknBt",
        "colab_type": "text"
      },
      "source": [
        "The `unsqueeze` method adds a singleton dimension, from a 1D tensor to a 2D tensor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKl-F1F0jAwY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        },
        "outputId": "c64fe1b6-228a-489e-d209-e18be699af09"
      },
      "source": [
        "target_unsqueezed = target.unsqueeze(1)\n",
        "target_unsqueezed"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[6],\n",
              "        [6],\n",
              "        [6],\n",
              "        ...,\n",
              "        [6],\n",
              "        [7],\n",
              "        [6]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    }
  ]
}