{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Real-World_Data_representations.ipynb",
      "provenance": [],
      "mount_file_id": "14LZm0HE1LCjwhr_oXQiPcP_tWpe9fv10",
      "authorship_tag": "ABX9TyMI+GOkqT1WZgWDLNN0qGvk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dqniellew1/DLPT/blob/master/Real_World_Data_representations.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4PyiJV8P9VG",
        "colab_type": "text"
      },
      "source": [
        "# Images\n",
        "\n",
        "`imageio` package to handle images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XV55UkiQYA9S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "drive_dir = 'drive/My Drive/dlwpt-code/data/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TwIJVKm7QFdD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import imageio"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hN3S_cwHQKEP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "60fcad9c-9623-4ea5-85b0-381b16d6cec2"
      },
      "source": [
        "# Loading an image\n",
        "img_arr = imageio.imread(drive_dir + '/p1ch4/image-dog/bobby.jpg')\n",
        "img_arr.shape"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(720, 1280, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTPewbZIS0Sm",
        "colab_type": "text"
      },
      "source": [
        "PyTorch require for input tensors to be in the format of `C x H x W`.\n",
        "We can use `permute` method to get the new format."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33nMKeMbSd3Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "img = torch.from_numpy(img_arr)\n",
        "out = img.permute(2, 0, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68_1_CN-TflE",
        "colab_type": "text"
      },
      "source": [
        "To store images in a batch in tensor, the dimensions are a `N x C x H x W` \n",
        "tensor.\n",
        "\n",
        "A more efficient alternative to using `stack` to build up the tensor, we can pre-allocate a tensor of approiprate size and fill it with images loaded from a directory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GzlyBSx3TJLL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 100\n",
        "batch = torch.zeros(100, 3, 256, 256, dtype=torch.uint8)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zy6LWZx4TMQ-",
        "colab_type": "text"
      },
      "source": [
        "## Loading images from a directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dlvm503eUbdf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "\n",
        "data_dir = 'drive/My Drive/dlwpt-code/data/p1ch4/image-cats/'\n",
        "filenames = [name for name in os.listdir(data_dir) if os.path.splitext(name)[-1] == '.png']\n",
        "for i, filename in enumerate(filenames):\n",
        "  img_arr = imageio.imread(os.path.join(data_dir, filename))\n",
        "  img_t = torch.from_numpy(img_arr)\n",
        "  img_t = img_t.permute(2, 0, 1)\n",
        "  img_t = img_t[:3] # Keep only the first three channels. Only RGB inputs.\n",
        "  batch[i] = img_t"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uz9vFJt3WAGS",
        "colab_type": "text"
      },
      "source": [
        "Neural networks work best when the input data ranges from 0 to 1, or from -1 to 1. Typically we want to cast a tensor to floating point and normalize the values of the pixels.\n",
        "\n",
        "Normalizations is trickier as we have to decide the range of input between (0 to 1) or (-1 to 1). One possibility is to just divide the values of pixels by 255. (The maximum nimber representable number in 8-bit unsigned)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9f6eddVRVO5n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch = batch.float()\n",
        "batch /= 255.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3F05bHIwWyLT",
        "colab_type": "text"
      },
      "source": [
        "Another way is to compute the **mean** and **std** of the input data and scale it so that the output has **zero mean** and **unit std** across each channel."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CiDbA4k4Wnn_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_channels = batch.shape[1]\n",
        "for c in range(n_channels):\n",
        "  mean = torch.mean(batch[:, c])\n",
        "  std = torch.std(batch[:, c])\n",
        "  batch[:, c] = (batch[:, c] - mean) / std"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAXYecOQWoP6",
        "colab_type": "text"
      },
      "source": [
        "Above is an example of normalizing a single image, because we do not know yet how to operate on an entire dataset. It is good practice to compute the mean and standard deviation on the entire training data in advance and then subtract and divide by these fixed pre-computed quantities."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qOtl1i2XUJz",
        "colab_type": "text"
      },
      "source": [
        "# Volumetric Data\n",
        "\n",
        "Consists of an added dimension after channel which is **depth**, leading to a 5D tensor of shape:\n",
        "\n",
        " `N x C x D x H x W`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPPeKKSHYxt7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82
        },
        "outputId": "4eb21adb-8afe-4bae-d24a-5647f6bef896"
      },
      "source": [
        "# Loading in a sample CT scan\n",
        "dir_path = drive_dir + \"p1ch4/volumetric-dicom/2-LUNG 3.0  B70f-04083\"\n",
        "vol_arr = imageio.volread(dir_path, 'DICOM')\n",
        "vol_arr.shape"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading DICOM (examining files): 1/99 files (1.0%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2/99 files (2.0%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b3/99 files (3.0%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b4/99 files (4.0%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b5/99 files (5.1%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b6/99 files (6.1%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b7/99 files (7.1%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b8/99 files (8.1%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b9/99 files (9.1%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/99 files (10.1%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/99 files (11.1%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/99 files (12.1%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13/99 files (13.1%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b14/99 files (14.1%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b15/99 files (15.2%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16/99 files (16.2%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b17/99 files (17.2%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b18/99 files (18.2%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b19/99 files (19.2%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20/99 files (20.2%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b21/99 files (21.2%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b22/99 files (22.2%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b23/99 files (23.2%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b24/99 files (24.2%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b25/99 files (25.3%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b26/99 files (26.3%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b27/99 files (27.3%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b28/99 files (28.3%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b29/99 files (29.3%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b30/99 files (30.3%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b31/99 files (31.3%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32/99 files (32.3%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b33/99 files (33.3%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b34/99 files (34.3%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b35/99 files (35.4%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b36/99 files (36.4%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b37/99 files (37.4%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b38/99 files (38.4%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b39/99 files (39.4%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b40/99 files (40.4%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b41/99 files (41.4%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b42/99 files (42.4%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b43/99 files (43.4%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b44/99 files (44.4%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b45/99 files (45.5%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b46/99 files (46.5%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b47/99 files (47.5%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b48/99 files (48.5%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b49/99 files (49.5%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b50/99 files (50.5%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b51/99 files (51.5%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b52/99 files (52.5%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b53/99 files (53.5%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b54/99 files (54.5%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b55/99 files (55.6%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b56/99 files (56.6%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b57/99 files (57.6%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b58/99 files (58.6%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b59/99 files (59.6%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b60/99 files (60.6%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b61/99 files (61.6%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b62/99 files (62.6%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b63/99 files (63.6%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b64/99 files (64.6%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b65/99 files (65.7%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b66/99 files (66.7%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b67/99 files (67.7%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b68/99 files (68.7%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b69/99 files (69.7%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b70/99 files (70.7%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b71/99 files (71.7%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b72/99 files (72.7%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b73/99 files (73.7%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b74/99 files (74.7%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b75/99 files (75.8%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b76/99 files (76.8%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b77/99 files (77.8%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b78/99 files (78.8%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b79/99 files (79.8%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b80/99 files (80.8%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b81/99 files (81.8%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b82/99 files (82.8%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b83/99 files (83.8%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b84/99 files (84.8%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b85/99 files (85.9%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b86/99 files (86.9%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b87/99 files (87.9%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b88/99 files (88.9%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b89/99 files (89.9%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b90/99 files (90.9%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b91/99 files (91.9%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b92/99 files (92.9%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b93/99 files (93.9%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b94/99 files (94.9%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b95/99 files (96.0%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b96/99 files (97.0%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b97/99 files (98.0%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b98/99 files (99.0%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b99/99 files (100.0%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b99/99 files (100.0%)\n",
            "  Found 1 correct series.\n",
            "Reading DICOM (loading data): 18/99  (18.2%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b49/99  (49.5%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b81/99  (81.8%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b99/99  (100.0%)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(99, 512, 512)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCQy0VeKbL3B",
        "colab_type": "text"
      },
      "source": [
        "In this case, the layout is different from what PyTorch expects, due to having no channel information. We will have to make room for the `channel` dimension using `unsqueeze`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z0nTJnFscDbV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "bac3dd64-b7d0-41fb-9e10-c692a03d5b2a"
      },
      "source": [
        "vol = torch.from_numpy(vol_arr).float()\n",
        "vol = torch.transpose(vol, 0, 2)\n",
        "vol = torch.unsqueeze(vol, 0)\n",
        "\n",
        "vol.shape"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 512, 512, 99])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cK6-uM2ncMt0",
        "colab_type": "text"
      },
      "source": [
        "# Tabular Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vi1VaWWucvVQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import csv\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5-XDZCPd__8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        },
        "outputId": "0bbf3a33-f0c3-4ab4-9829-85eac0bcb23b"
      },
      "source": [
        "wine_path = drive_dir + 'p1ch4/tabular-wine/winequality-white.csv'\n",
        "wineq_numpy = np.loadtxt(wine_path, dtype=np.float32, delimiter=';', skiprows=1)\n",
        "wineq_numpy"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 7.  ,  0.27,  0.36, ...,  0.45,  8.8 ,  6.  ],\n",
              "       [ 6.3 ,  0.3 ,  0.34, ...,  0.49,  9.5 ,  6.  ],\n",
              "       [ 8.1 ,  0.28,  0.4 , ...,  0.44, 10.1 ,  6.  ],\n",
              "       ...,\n",
              "       [ 6.5 ,  0.24,  0.19, ...,  0.46,  9.4 ,  6.  ],\n",
              "       [ 5.5 ,  0.29,  0.3 , ...,  0.38, 12.8 ,  7.  ],\n",
              "       [ 6.  ,  0.21,  0.38, ...,  0.32, 11.8 ,  6.  ]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3cPq9vceWaV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "outputId": "f16e256c-e99c-40c2-8f1e-157722fc84e1"
      },
      "source": [
        "col_list = next(csv.reader(open(wine_path), delimiter=';'))\n",
        "wineq_numpy.shape, col_list"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((4898, 12),\n",
              " ['fixed acidity',\n",
              "  'volatile acidity',\n",
              "  'citric acid',\n",
              "  'residual sugar',\n",
              "  'chlorides',\n",
              "  'free sulfur dioxide',\n",
              "  'total sulfur dioxide',\n",
              "  'density',\n",
              "  'pH',\n",
              "  'sulphates',\n",
              "  'alcohol',\n",
              "  'quality'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRK_2_mYelAa",
        "colab_type": "text"
      },
      "source": [
        "Convert the NumPy array into a PyTorch tensor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ua8W6OB2e7wk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "0d07bb69-3ba1-4b65-cb4d-035568e1c26f"
      },
      "source": [
        "wineq = torch.from_numpy(wineq_numpy)\n",
        "\n",
        "wineq.shape, wineq.dtype"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([4898, 12]), torch.float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crSMSMchfCHJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        },
        "outputId": "b0bfa33d-f765-4a0a-f244-846b3301cda4"
      },
      "source": [
        "data = wineq[:, :-1]\n",
        "data, data.shape"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[ 7.0000,  0.2700,  0.3600,  ...,  3.0000,  0.4500,  8.8000],\n",
              "         [ 6.3000,  0.3000,  0.3400,  ...,  3.3000,  0.4900,  9.5000],\n",
              "         [ 8.1000,  0.2800,  0.4000,  ...,  3.2600,  0.4400, 10.1000],\n",
              "         ...,\n",
              "         [ 6.5000,  0.2400,  0.1900,  ...,  2.9900,  0.4600,  9.4000],\n",
              "         [ 5.5000,  0.2900,  0.3000,  ...,  3.3400,  0.3800, 12.8000],\n",
              "         [ 6.0000,  0.2100,  0.3800,  ...,  3.2600,  0.3200, 11.8000]]),\n",
              " torch.Size([4898, 11]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ARVsBRX1hMiM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "620fd450-1581-49da-bd75-956cfcfc1eca"
      },
      "source": [
        "target = wineq[:,-1]\n",
        "target, target.shape"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([6., 6., 6.,  ..., 6., 7., 6.]), torch.Size([4898]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TjrpyCmLhPLd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "0de51798-1f70-41bb-c689-e08311d0df06"
      },
      "source": [
        "# Treating labels as an integer vector of scores:\n",
        "target = wineq[:, -1].long()\n",
        "target"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([6, 6, 6,  ..., 6, 7, 6])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTqU8lXghu-j",
        "colab_type": "text"
      },
      "source": [
        "In PyTorch we can achieve `one-hot` encoding using the `scatter_` method, which fills the tensor with values from a source tensor along the indices provided as arguments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1yv52xUFk4Vy",
        "colab_type": "text"
      },
      "source": [
        "The `scatter_` method reads plainly as for each row, take the index of the target label (which conincides with the score in our case) and use it as the column index to set the value to 1.0. The end-result is a tensor encoding categorical information."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-TzLR1qh6nX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        },
        "outputId": "081b853c-2d41-474e-ed1f-68d565414c8d"
      },
      "source": [
        "target_onehot = torch.zeros(target.shape[0], 10)\n",
        "\n",
        "target_onehot.scatter_(1, target.unsqueeze(1), 1.0)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSbs5dTqknBt",
        "colab_type": "text"
      },
      "source": [
        "The `unsqueeze` method adds a singleton dimension, from a 1D tensor to a 2D tensor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKl-F1F0jAwY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        },
        "outputId": "494291e0-0f3b-4dbe-d2b7-6277c0c4a70c"
      },
      "source": [
        "target_unsqueezed = target.unsqueeze(1)\n",
        "target_unsqueezed"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[6],\n",
              "        [6],\n",
              "        [6],\n",
              "        ...,\n",
              "        [6],\n",
              "        [7],\n",
              "        [6]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wy--nybCwUl1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "90250ed9-4faa-484e-93a9-e568624f2ac4"
      },
      "source": [
        "# Compute the means across rows going down, therefore we get 11 means for the 11 columns\n",
        "data_mean = torch.mean(data, dim=0)\n",
        "data_mean"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([6.8548e+00, 2.7824e-01, 3.3419e-01, 6.3914e+00, 4.5772e-02, 3.5308e+01,\n",
              "        1.3836e+02, 9.9403e-01, 3.1883e+00, 4.8985e-01, 1.0514e+01])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qqPbNpgONASq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "1a166c0e-2265-438c-cbfd-35f96f986fb8"
      },
      "source": [
        "# Variance\n",
        "data_var = torch.var(data, dim=0)\n",
        "data_var"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([7.1211e-01, 1.0160e-02, 1.4646e-02, 2.5726e+01, 4.7733e-04, 2.8924e+02,\n",
              "        1.8061e+03, 8.9455e-06, 2.2801e-02, 1.3025e-02, 1.5144e+00])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xNIFnUwNhCc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "outputId": "5d32712b-4507-48a5-f557-ff569b60c099"
      },
      "source": [
        "data_normalized = (data - data_mean) / torch.sqrt(data_var)\n",
        "data_normalized"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 1.7209e-01, -8.1764e-02,  2.1325e-01,  ..., -1.2468e+00,\n",
              "         -3.4914e-01, -1.3930e+00],\n",
              "        [-6.5743e-01,  2.1587e-01,  4.7991e-02,  ...,  7.3992e-01,\n",
              "          1.3467e-03, -8.2418e-01],\n",
              "        [ 1.4756e+00,  1.7448e-02,  5.4378e-01,  ...,  4.7502e-01,\n",
              "         -4.3677e-01, -3.3662e-01],\n",
              "        ...,\n",
              "        [-4.2042e-01, -3.7940e-01, -1.1915e+00,  ..., -1.3131e+00,\n",
              "         -2.6152e-01, -9.0544e-01],\n",
              "        [-1.6054e+00,  1.1666e-01, -2.8253e-01,  ...,  1.0048e+00,\n",
              "         -9.6250e-01,  1.8574e+00],\n",
              "        [-1.0129e+00, -6.7703e-01,  3.7852e-01,  ...,  4.7502e-01,\n",
              "         -1.4882e+00,  1.0448e+00]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CuREAmbHNinN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "5d545c45-8f33-4854-d65b-c7d16bbd477e"
      },
      "source": [
        "data_normalized.shape"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4898, 11])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Hw75KngOTkK",
        "colab_type": "text"
      },
      "source": [
        "Determine which rows in `target` correspond to a score less than or equal to 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WiCp7eTxOj4j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "1eeb1915-1346-4c9f-aa85-12d609978712"
      },
      "source": [
        "bad_indexes = target <= 3\n",
        "bad_indexes.shape, bad_indexes.dtype, bad_indexes.sum()"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([4898]), torch.bool, tensor(20))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zx67_LMZOrvE",
        "colab_type": "text"
      },
      "source": [
        "We can use PyTorch's advanced indexing to filter `data` "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RubfiVhTPCBr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "e6af9c85-a6bc-42b7-9838-cc3320892f75"
      },
      "source": [
        "bad_data = data[bad_indexes]\n",
        "bad_data.shape"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([20, 11])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJFm4WcaPF5U",
        "colab_type": "text"
      },
      "source": [
        "Get information from different groups of wine."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3kBEoo6CPSAU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bad_data = data[target <= 3]\n",
        "mid_data = data[(target > 3) & (target < 7)]\n",
        "good_data = data[target >= 7]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XiBNPCUaPa5-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bad_mean = torch.mean(bad_data, dim=0)\n",
        "mid_mean = torch.mean(mid_data, dim=0)\n",
        "good_mean = torch.mean(good_data, dim=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOzxObvdPlHC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "outputId": "178f3351-bbc6-4441-8761-96021d1c63aa"
      },
      "source": [
        "print(\"{:2} {:20} {:^6} {:^6} {:^6}\".format(\"No\", \"Columns\", \"Bad\", \"Mid\", \"Good\"))\n",
        "for i, args in enumerate(zip(col_list, bad_mean, mid_mean, good_mean)):\n",
        "  print(\"{:2} {:20} {:6.2f} {:6.2f} {:6.2f}\".format(i, *args))"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No Columns               Bad    Mid    Good \n",
            " 0 fixed acidity          7.60   6.89   6.73\n",
            " 1 volatile acidity       0.33   0.28   0.27\n",
            " 2 citric acid            0.34   0.34   0.33\n",
            " 3 residual sugar         6.39   6.71   5.26\n",
            " 4 chlorides              0.05   0.05   0.04\n",
            " 5 free sulfur dioxide   53.33  35.42  34.55\n",
            " 6 total sulfur dioxide 170.60 141.83 125.25\n",
            " 7 density                0.99   0.99   0.99\n",
            " 8 pH                     3.19   3.18   3.22\n",
            " 9 sulphates              0.47   0.49   0.50\n",
            "10 alcohol               10.34  10.26  11.42\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YZkusLvSu62",
        "colab_type": "text"
      },
      "source": [
        "Here we are using the threshold on total sulfur dioxide to discriminate good wines from bad ones."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zaH-Xf8_QS0o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "23942369-034c-4f4a-e1af-4fe8a9cb47b6"
      },
      "source": [
        "total_sulfur_threshold = 141.83\n",
        "total_sulfur_data = data[:, 6]\n",
        "predicted_indexes = torch.lt(total_sulfur_data, total_sulfur_threshold)\n",
        "\n",
        "predicted_indexes.shape, predicted_indexes.dtype, predicted_indexes.sum()"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([4898]), torch.bool, tensor(2727))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FNPamaiSp3F",
        "colab_type": "text"
      },
      "source": [
        "Get indexes of actually good wine"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fZs8G0VSYh0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "cf89eda3-1e83-4f18-8989-f8e58f81a93a"
      },
      "source": [
        "actual_indexes = target > 5\n",
        "\n",
        "actual_indexes.shape, actual_indexes.dtype, actual_indexes.sum()"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([4898]), torch.bool, tensor(3258))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8zaTzUBTB-V",
        "colab_type": "text"
      },
      "source": [
        "Using threshold on sulfur dioxide to determine whether we have good wine, our actual indexes has 500 more than the predicted index.\n",
        "\n",
        "Now we need to see how well our predictions line up with the actual rankings. We will perfrom a logical \"and\" between our prediction indexes and the actual good indexes, and use that intersection of wines-in-agreement to determine how well we did."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QoNsEIvVTtGP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_matches = torch.sum(actual_indexes & predicted_indexes).item()\n",
        "n_predicted = torch.sum(predicted_indexes).item()\n",
        "n_actual = torch.sum(actual_indexes).item()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j38szGuXTy4T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "7e5441be-6d57-458d-fe38-eabcda056d99"
      },
      "source": [
        "n_matches, n_matches / n_predicted, n_matches / n_actual"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2018, 0.74000733406674, 0.6193984039287906)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQHzSyvrTzn3",
        "colab_type": "text"
      },
      "source": [
        "We got about 2000 wines right. Since we have 2700 wines predicted, this gives us a 74% chanve thst if we predict a wine to be high quality, it actually is. Unfortunately, there are 3200 good wines, but we only identified 61% of them. That's just slightly better than random."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sW2Jty6dUs05",
        "colab_type": "text"
      },
      "source": [
        "# Time series data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8QlTO2WaB4w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "outputId": "16ca469c-2877-4516-f12c-65cebacb7250"
      },
      "source": [
        "bikes_numpy = np.loadtxt(drive_dir + \"p1ch4/bike-sharing-dataset/hour-fixed.csv\",\n",
        "                         dtype=np.float32,\n",
        "                         delimiter=\",\",\n",
        "                         skiprows=1,\n",
        "                         converters={1: lambda x: float(x[8:10])}) # Convert date strings to numbers corresponding to the day of the month in column 1.\n",
        "\n",
        "bikes = torch.from_numpy(bikes_numpy)\n",
        "bikes"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 3.0000e+00, 1.3000e+01,\n",
              "         1.6000e+01],\n",
              "        [2.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 8.0000e+00, 3.2000e+01,\n",
              "         4.0000e+01],\n",
              "        [3.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 5.0000e+00, 2.7000e+01,\n",
              "         3.2000e+01],\n",
              "        ...,\n",
              "        [1.7377e+04, 3.1000e+01, 1.0000e+00,  ..., 7.0000e+00, 8.3000e+01,\n",
              "         9.0000e+01],\n",
              "        [1.7378e+04, 3.1000e+01, 1.0000e+00,  ..., 1.3000e+01, 4.8000e+01,\n",
              "         6.1000e+01],\n",
              "        [1.7379e+04, 3.1000e+01, 1.0000e+00,  ..., 1.2000e+01, 3.7000e+01,\n",
              "         4.9000e+01]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SqSwDWIsgypT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "0072c5a8-1846-43be-d8f3-450a36240978"
      },
      "source": [
        "bikes.shape, bikes.stride()"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([17520, 17]), (17, 1))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvmP-TyBhTMc",
        "colab_type": "text"
      },
      "source": [
        "Right now, we have the data in 17520 hours across 17 columns, we will reshape the data to have three axes; day, hour and then our 17 columns."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wC50aLUciGk1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "136744cb-659f-43d2-9938-e13e3952af5c"
      },
      "source": [
        "# -1 acts as a placeholder for \"however many indexes are left\" given the other dimensions and the original number of elements\n",
        "daily_bikes = bikes.view(-1, 24, bikes.shape[1]) \n",
        "daily_bikes.shape, daily_bikes.stride()"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([730, 24, 17]), (408, 17, 1))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awZ06wiij5zB",
        "colab_type": "text"
      },
      "source": [
        "Transpose daily_bikes to meet the `N x C x L` format"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKBPxCcsiV86",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "7e58b54a-4379-458b-abee-3a7b106ed5e7"
      },
      "source": [
        "daily_bikes = daily_bikes.transpose(1, 2)\n",
        "daily_bikes.shape, daily_bikes.stride()\n"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([730, 17, 24]), (408, 1, 17))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wt_mUMHQiYc0",
        "colab_type": "text"
      },
      "source": [
        "In order to make it easier to render our data, we are only using the first day."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQ6G8xKAkQRy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "first_day = bikes[:24].long()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQQ_RLupkTNE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "weather_onehot = torch.zeros(first_day.shape[0], 4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7sIiZfmtkUSF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "732748e6-4f13-494f-c34a-e21b0ad163a1"
      },
      "source": [
        "first_day[:, 9]"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 2, 2, 2, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jftvNqmfkjnH",
        "colab_type": "text"
      },
      "source": [
        "Scatter ones into our matrix according to the corresponding level at each row. Remember the use of `unsqueeze` to add a singleton dimension as we did."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Lddgpelk1Fx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "outputId": "17dc563a-c37f-40fb-d0b9-d9377a335b22"
      },
      "source": [
        "weather_onehot.scatter_(dim=1,\n",
        "                        index=first_day[:, 9].unsqueeze(1).long() - 1,\n",
        "                        value=1.0)"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 0., 0., 0.],\n",
              "        [1., 0., 0., 0.],\n",
              "        [1., 0., 0., 0.],\n",
              "        [1., 0., 0., 0.],\n",
              "        [1., 0., 0., 0.],\n",
              "        [0., 1., 0., 0.],\n",
              "        [1., 0., 0., 0.],\n",
              "        [1., 0., 0., 0.],\n",
              "        [1., 0., 0., 0.],\n",
              "        [1., 0., 0., 0.],\n",
              "        [1., 0., 0., 0.],\n",
              "        [1., 0., 0., 0.],\n",
              "        [1., 0., 0., 0.],\n",
              "        [0., 1., 0., 0.],\n",
              "        [0., 1., 0., 0.],\n",
              "        [0., 1., 0., 0.],\n",
              "        [0., 1., 0., 0.],\n",
              "        [0., 1., 0., 0.],\n",
              "        [0., 0., 1., 0.],\n",
              "        [0., 0., 1., 0.],\n",
              "        [0., 1., 0., 0.],\n",
              "        [0., 1., 0., 0.],\n",
              "        [0., 1., 0., 0.],\n",
              "        [0., 1., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxVCpL4jlP5J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}